#ELKStack
官网：https://www.elastic.co
插件文档：https://www.elastic.co/guide/en/logstash-versioned-plugins/current/index.html
ELKStack简介:
对于日志来说，最常见的需求就是收集、存储、查询、展示，开源社区正好有相对应的开源项目：logstash（收集）、elasticsearch（存储+搜索）、kibana（展示），我们将这三个组合起来的技术称之为ELKStack，所以说ELKStack指的是Elasticsearch、Logstash、Kibana技术栈的结合

Elasticsearch天生是分布式的，有两种方式进行通信：1.组播（加到组中，在组中的主机互相通信） 2.单播(指定主机)

<pre>
安装JDK
[root@clusterFS-node4-salt ~]# yum install -y java-1.8.0
[root@clusterFS-node4-salt ~]# java -version
openjdk version "1.8.0_191"
OpenJDK Runtime Environment (build 1.8.0_191-b12)
OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)
##Elasticsearch部署
Elasticsearch首先需要Java环境，所以需要提前安装好JDK，可以直接使用yum安装。也可以从Oracle官网下载JDK进行安装。开始之前要确保JDK正常安装并且环境变量也配置正确：
YUM安装ElasticSearch
1.下载并安装GPG key:
[root@clusterFS-node4-salt ~]# rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
2.添加yum仓库:
[root@clusterFS-node4-salt ~]# vim /etc/yum.repos.d/elasticsearch.repo
[elasticsearch-2.x]
name=Elasticsearch repository for 2.x packages
baseurl=http://packages.elastic.co/elasticsearch/2.x/centos
gpgcheck=1
gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
enabled=1
3.安装elasticsearch:
[root@clusterFS-node4-salt ~]#  yum install -y elasticsearch
4.更改elasticsearch.yml配置文件：
[root@clusterFS-node4-salt elasticsearch]# vim elasticsearch.yml
[root@clusterFS-node4-salt elasticsearch]# grep '^[a-Z]' elasticsearch.yml
cluster.name: myes
node.name: elk-node1
path.data: /data/es-data
path.logs: /var/log/elasticsearch
bootstrap.memory_lock: true  #锁住物理内存不适用虚拟内存
network.host: 192.168.1.31
http.port: 9200
5.配置/data目录给elasticsearch权限：
[root@clusterFS-node4-salt ~]# chown -R elasticsearch:elasticsearch /data
6.启动elasticsearch:
[root@clusterFS-node4-salt elasticsearch]# systemctl start elasticsearch.service
7.测试elasticsearch服务是否正常：
[root@clusterFS-node4-salt ~]# curl -i -XGET "http://192.168.1.31:9200/_count"
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 59

{"count":0,"_shards":{"total":0,"successful":0,"failed":0}}
注：java使用得是lucene，lucene是搜索巨头，大部分搜索基本都是lucene，lucene本身没有集群，高可用，分片。而elasticsearch具备集群，高可用和分片，elasticsearch底层是lucene。elasticsearch跟glusterFS一样，分布式是无中心的。
8.装插件，用插件联系elasticsearch[也可以用api]:
[root@clusterFS-node4-salt ~]# /usr/share/elasticsearch/bin/plugin install mobz/elasticsearch-head  #直接从github上去抓取安装
[root@clusterFS-node4-salt ~]# /usr/share/elasticsearch/bin/plugin install lmenezes/elasticsearch-kopf  #直接从github上去抓取安装
lukas-vlcek/bigdesk#bigdesk插件直接从github上抓取，但是版本不支持
[root@clusterFS-node4-salt ~]# /usr/share/elasticsearch/bin/plugin install marvel-agent  #从官网上去抓取
9.测试插件kopf:
http://192.168.1.31:9200/_plugin/kopf/ 
10.访问插件head[用于集群管理]:
http://192.168.1.31:9200/_plugin/head/
10.1：
点击复合查询-输入/index-demo/test-选择POST-输入json信息并提交请求
json信息：
{
  "user": "oldboy",
  "mesg": "hello world"
}
输出：
{

    "_index": "index-demo",
    "_type": "test",
    "_id": "AWkvYTZQKeOJs7st2toX",
    "_version": 1,
    "_shards": {
        "total": 2,
        "successful": 1,
        "failed": 0
    },
    "created": true

}
10.2:
点击概览-可查看到0 1 2 3 4 这些分片-深黑色是主分片浅灰色是副本分片-主分片和负分片要部署在不同的机器上-点击连接-集群健康值为黄色为良好，为红色是主负本分片都丢失了
注意：在生产环境中，http://192.168.1.31:9200/_plugin/head/打开要5分钟，因为elasticsearch要去收集日志并整理，要花时间所致。
11.访问插件kopf:
http://192.168.1.31:9200/_plugin/kopf/
12.另外一个节点也要安装elasticsearch,集群名要一样，节点不一样，此时有两个es了，两个节点根据组播查找到另外节点【也可以用单播查找到另外节点】。两个节点互相查找到以后会进行选举，产生主节点和备节点，主节点和备节点对用户来说不重要，随便哪一个节点都可以转发信息。分片来说，只能是主分片等划分，副本分片不行。
13.由于elasticsearch组播无法查询到另外节点，此时用单播方式，[root@clusterFS-node3-salt ~]# vim /etc/elasticsearch/elasticsearch.yml
discovery.zen.ping.unicast.hosts: ["192.168.1.31", "192.168.1.37"]    #ip地址可以加端口。默认是9200
14.[root@clusterFS-node3-salt ~]# systemctl restart elasticsearch.service
15.网页查看http://192.168.1.31:9200/_plugin/head/，此时有两个节点，带"五角星"的为主节点。健康值也为绿色。
16.可以用curl来查看集群的健康状态：curl -XGET http://192.168.1.31:9200/_cluster/health?pretty=true
[root@clusterFS-node3-salt .ssh]# curl -XGET http://192.168.1.31:9200/_cluster/health?pretty=true  #?pretty=true是要漂亮的输出
{
  "cluster_name" : "myes",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 5,
  "active_shards" : 10,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
17._cat来查看：
[root@clusterFS-node3-salt .ssh]# curl -XGET http://192.168.1.31:9200/_cat
=^.^=
/_cat/allocation
/_cat/shards
/_cat/shards/{index}
/_cat/master
/_cat/nodes
/_cat/indices
/_cat/indices/{index}
/_cat/segments
/_cat/segments/{index}
/_cat/count
/_cat/count/{index}
/_cat/recovery
/_cat/recovery/{index}
/_cat/health
/_cat/pending_tasks
/_cat/aliases
/_cat/aliases/{alias}
/_cat/thread_pool
/_cat/plugins
/_cat/fielddata
/_cat/fielddata/{fields}
/_cat/nodeattrs
/_cat/repositories
/_cat/snapshots/{repository}
18.[root@clusterFS-node3-salt .ssh]# curl -XGET http://192.168.1.31:9200/_cat/nodes
192.168.1.31 192.168.1.31 7 97 0.14 d * els-node1
192.168.1.37 192.168.1.37 7 96 0.00 d m els-node2
19.生产部署硬件：一般内存64G，JVM不要超过32G，SSD硬盘最好。不要调度。CPU越多越好，网卡越块越好。JVM版本越高越好。
20.[root@clusterFS-node3-salt .ssh]# cat /proc/sys/vm/max_map_count
65530
21.上elasticsearch第一件事情就是改openfile:sysctl -w vm.max_map_count=262144

#logstash部署：
YUM部署LogStash:
1.下载并安装GPG key:
rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
2.添加yum仓库:
vim /etc/yum.repos.d/logstash.repo
[logstash-2.3]
name=Logstash repository for 2.3.x packages
baseurl=https://packages.elastic.co/logstash/2.3/centos
gpgcheck=1
gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearch
enabled=1
3.安装logstash:
yum install -y logstash
4.启动logstash:
systemctl start logstash

测试使用：
1.[root@clusterFS-node3-salt ~]# curl -i -XGET http://192.168.1.31:9200/_cluster/health?pretty=true
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 458

{
  "cluster_name" : "myes",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 5,
  "active_shards" : 10,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
2.[root@clusterFS-node3-salt ~]# /opt/logstash/bin/logstash -e 'input { stdin {} } output { stdout {} }' #从标准输入到标准输出,所以下面直接输出在屏幕上
Settings: Default pipeline workers: 2
Pipeline main started
hello
2019-03-01T08:06:19.817Z clusterFS-node3-salt hello
3.[root@clusterFS-node3-salt ~]# /opt/logstash/bin/logstash -e 'input { stdin {} } output { stdout { codec => rubydebug } }' #使用codec=>rubydebug使输出更美观
Settings: Default pipeline workers: 2
Pipeline main started
aa
{
       "message" => "aa",
      "@version" => "1",
    "@timestamp" => "2019-03-01T08:09:33.108Z",
          "host" => "clusterFS-node3-salt"
}
4.[root@clusterFS-node3-salt ~]# /opt/logstash/bin/logstash -e 'input { stdin {} } output { elasticsearch { hosts=>[ "192.168.1.31:9200" ] index=>"logstash-%{+YYYY.MM.dd}" } }'   #输出到elasticsearch
5.[root@clusterFS-node3-salt ~]# /opt/logstash/bin/logstash -e 'input { stdin {} } output { stdout { codec => rubydebug }  elasticsearch { hosts=>[ "192.168.1.31:9200" ] index=>"logstash-%{+YYYY.MM.dd}" } }'  #输出到elasticsearch中和标准输出
6.logstash脚本放置在/etc/logstash/conf.d/下，因为/etc/init.d/logstash里面LS_CONF_DIR=/etc/logstash/conf.d配置了从这里读。
7.cd /etc/logstash/conf.d && vim demo.conf
input{
    stdin{}
}

filter{
}

output{
##号代表注释
    elasticsearch {
        hosts => ["192.168.1.31:9200"]
        index => "logstash-%{+YYYY.MM.dd}"
    }
    stdout{
        codec => rubydebug
    }
}
8.[root@clusterFS-node3-salt conf.d]# /opt/logstash/bin/logstash -f /etc/logstash/conf.d/demo.conf  #命令运行这个配置文件就可以输出到elasticsearch和logstash中了
9.logstash语法：
1.行 = 事件	2.input  output		3.事件 ->  input -> codec -> filter -> codec -> output(编解码的动作)
10.input模块：
file插件收集系统日志：
input{
    file {
        path => ["/var/log/messages", "/var/log/secure"]
        type => "system-log"
        start_position => "beginning"
    }
}

filter{
}

output{
    elasticsearch {
        hosts => ["192.168.1.31:9200"]
        index => "system-log-%{+YYYY.MM}"
    }
}

#kabana部署:
kabana跟logstash没有一点关系，kabana只是为elasticsearch设置的一个设置界面
Yum安装Kibana
1.下载并安装GPG key
[root@clusterFS-node4-salt log]#  rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
2.添加yum仓库
[root@clusterFS-node4-salt log]# cat /etc/yum.repos.d/kibana.repo
[kibana-4.5]
name=Kibana repository for 4.5.x packages
baseurl=http://packages.elastic.co/kibana/4.5/centos
gpgcheck=1
gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
enabled=1
3.安装kibana
[root@clusterFS-node4-salt log]# yum install -y kibana 
4.编辑kibana配置文件
[root@clusterFS-node4-salt log]# vim /opt/kibana/config/kibana.yml
[root@clusterFS-node4-salt log]# grep '^[a-Z]' /opt/kibana/config/kibana.yml
server.port: 5601
server.host: "0.0.0.0"
elasticsearch.url: "http://192.168.1.31:9200"
kibana.index: ".kibana"
5.http://192.168.1.31:5601打开kibana,进入setting菜单添加index索引，勾上{Use event times to create index names [DEPRECATED] }选项，进入Index name or pattern选项，设置index索引匹配名称[logstash-]YYYY.MM.DD
6.进入discovery菜单进行日志的查看及管理

7.测试java日志提交es-log到新的索引
vim /etc/logstash/conf.d/file.conf
编写配置格式：每行4个空格
input{
    file {
        path => "/var/log/elasticsearch/myes.log"
        type => "es-log"
        start_position => "beginning"
    }
    file {
        path => ["/var/log/secure", "/var/log/messages"]
        type => "system-log"
        start_position => "beginning"
    }
}

filter{
}

output{
    if [type] == "es-log" {    #使用if来判断type
        elasticsearch {
            hosts => ["192.168.1.31:9200"]
            index => "es-log-%{+YYYY.MM}"
        }
    }
    if [type] == "system-log" {
        elasticsearch {
            hosts => ["192.168.1.31:9200"]
            index => "system-log-%{+YYYY.MM}"
        }
    }
}

需求：由于java日志报错exception时有多行是一个事件，而logstash却认为是多行而不是一行，所以要用multiline插件来更改使logstash认为这是一个事件输出为一行
8.测试配置文件：
[root@clusterFS-node4-salt conf.d]# cat codec.conf
input{
    stdin {
        codec => multiline{
            pattern => "^\["
            negate => true
            what => "previous"
        }
    }
}

filter{
}

output{
    stdout {
        codec => rubydebug
    }
}

9.加入正式配置文件：
[root@clusterFS-node4-salt conf.d]# cat file.conf
input{
    file {
        path => "/var/log/elasticsearch/myes.log"
        type => "es-log"
        start_position => "beginning"
        codec => multiline{
            pattern => "^\["   #匹配到[开关的
            negate => true    #为真时
            what => "previous"   #之前的合并
        }

    }
    file {
        path => ["/var/log/secure", "/var/log/messages"]
        type => "system-log"
        start_position => "beginning"
    }
}

filter{
}

output{
    if [type] == "es-log" {
        elasticsearch {
            hosts => ["192.168.1.31:9200"]
            index => "es-log-%{+YYYY.MM}"
        }
    }
    if [type] == "system-log" {
        elasticsearch {
            hosts => ["192.168.1.31:9200"]
            index => "system-log-%{+YYYY.MM}"
        }
    }
}

10.删除~目录下.sincedb开头的隐藏文件（用户记录index索引收集到哪一行的数据），重新收集日志
11.运行/opt/logstash/bin/logstash -f /etc/logstash/conf.d/file.conf重新创建index，并且使java报的Exception都在一行显示，不会错误的显示多行
注意：建立索引必需使用/opt/logstash/bin/logstash -f /etc/logstash/conf.d/file.conf来建立，重启服务并不会建立索引。(由于启动/etc/init.d/logstash restart 而有些日志没有写到输出的地方，原因是有些日志文件logstash这个用户没有权限读取。)

12.收集nginx日志：
logstash使用json插件来收集nginx日志，排版nginx日志取得需要的字段信息。因为nginx日志默认并不是json格式，好在nginx支持日志改成json格式。
例如：
 ab -n 1000 -c 1 http://192.168.1.31/  #-n为请求数据，-c为并发数（concurrency）
less /var/log/nginx/access.log  #查看nginx访问日志，日志格式在/etc/nginx/nginx.conf配置文件里面log_format参数下
 log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                   '$status $body_bytes_sent "$http_referer" '
                   '"$http_user_agent" "$http_x_forwarded_for"';   #前面是系统自带日志参数
实现json格式两种方法：
第一种方式：nginx日志改成json格式:
log_format  access_log_json 	'{"user_ip":"$http_x_real_ip","lan_ip":"$remote_addr","log_time    	":"$time_iso8601","user_req":"$request","http_code":"$status","body_bytes_sent":"	$body_bytes_se    nt","req_time":"$request_time","user_ua":"$http_user_agent"} ';
#access_log_json为自己加进去的json格式
 access_log  /var/log/nginx/access_log_json.log  access_log_json;  #调用access_log_json日志格式 
第二种方式：
文件通过Redis直接收取，然后Python脚本读取Redis,写成Json,写入elasticsearch。
测试json日志格式：
input {
    file {
        path => "/var/log/nginx/access_log_json.log"
        codec => "json"
    }
}

filter {}

output{
    stdout {
        codec => rubydebug
    }
}

加入nginx.conf到/etc/logstash/conf.d/下：
[root@clusterFS-node4-salt conf.d]# cat nginx.conf
input {
    file {
        path => "/var/log/nginx/access_log_json.log"
        codec => "json"
        type => "nginx-access-log"
    }
}

filter {}

output{
    elasticsearch {
        hosts => ["192.168.1.31:9200"]
        index => "nginx-access-log-%{+YYYY.MM.dd}"
    }
}

13./opt/logstash/bin/logstash -f /etc/logstash/conf.d/nginx.conf #加-t是测试配置文件
注：ls -a /var/lib/logstash/  #logstash的sincedb默认在这个路径下
14.kibana中加nginx-access-log的index,之后可以在discovery下使用自己定义的json字段进行排序了。

14.在kibana中添加dashboard仪表盘，有makedown语法的（常用作紧急联系人）、计算器总计、饼图、柱状图、搜索结果等添加到仪表盘中。设置参数时Aggregation选项添加terms、Field 再可以添加自己配置的json键名。
注：当设置图表参数时显示?时，是index添加有问题，删除重新添加即可
生产环境的部署情况：
1.每个ES上面都启动一个Kibana
2.Kibana都连自己的ES
3.前端Nginx做负载均衡（kibana性能到20多个人时就极限）、ip_hast、身份验证（限制访问）、ACL。

##Rsyslog日志(Redhat6之后不叫syslog了) #shadowsocksVPN
syslog插件：logstash开启514端口，其他节点就可以把所有系统日志传到这台主机的514端口了。
##syslog插件 （logstash自带所有插件，免安装）

1.vim /etc/logstash/conf.d/syslog.conf
input {
    syslog{   #意思是开启收集系统日志的端口，用于收取其他节点的系统日志
        type => "system-syslog"
        port => 514
    }

}

output {
    stdout {
        codec => rubydebug
    }
}

2.在需要传送系统日志的机器/etc/rsyslog.conf下添加这行配置
*.* @@192.168.1.31:514  
3.重启rsyslog服务：
systemctl restart rsyslog.service

4.写入到正式配置文件：
[root@clusterFS-node4-salt conf.d]# cat syslog.conf
input {
    syslog{
        type => "system-syslog"
        port => 514
    }

}

output {
    elasticsearch {
        hosts => ["192.168.1.31:9200"]
        index => "system-syslog-%{+YYYY.MM}"   #YYYY.MM前有个+号
    }
    stdout {
        codec => rubydebug
    }
}

#TCP日志（使用tcp插件）
当logstash收集日志时少收了一些日志，要补日志到kibana上，有两种方法：
1. 把缺少日志写到文件，再通过logstash把文件传到kibana
2. 用tcp来传少的日志到kibana(这种方式较灵活)
tcp插件：
1. [root@clusterFS-node4-salt conf.d]# cat tcp.conf
input {
    tcp {   #开启tcp端口收集日志
        type => "tcp"
        port => 6666
        mode => "server"
    }
}

output {
    stdout{
        codec => rubydebug
    }
}
2. /opg/logstash/bin/logstash -f tcp.conf
3. 通过任意一种方式连接端口传送日志到logstash
echo "hehe" | nc 192.168.1.31 6666
nc 192.168.1.31 6666 < /etc/resolv.conf
echo "hello" > /dev/tcp/192.168.1.31/6666
 
##filter模块：
#grok插件：
1. grok系统自带正则表达式目录（自带大部分应用正则表达式）：
[root@clusterFS-node4-salt conf.d]# cd /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-2.0.5/patterns/
2. [root@clusterFS-node4-salt conf.d]# cat grok.conf
input {
        stdin {}
}

filter {
        grok {
                match => { "message" => "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}" }
        }
}

output {
        stdout {
                codec => rubydebug
        }
}
3. /opt/logstash/bin/logstash -f /etc/logstash/conf.d/grok.conf
4. 输入55.3.244.1 GET /index.html 15824 0.043进行测试正则表达式测试的结果。
5. 使用grok自带的正没表达式变量来过滤日志
[root@clusterFS-node4-salt conf.d]# cat apache-grok.conf
input {
        file{
                path => "/var/log/httpd/access_log"
                start_position => "beginning"
                type => "apache-accesslog"
        }
}

filter {
        grok{
                match => { "message" => "%{COMBINEDAPACHELOG}" }  #COMBINEDAPACHELOG为grok自带的正则表达式变量，变量在/opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-2.0.5/patterns/grok-patterns文件中
        }
}

output{
        elasticsearch{
                hosts => "192.168.1.31:9200"
                index => "apache-accesslog-%{+YYYY.MM}"
        }
}

/opt/logstash/bin/logstash -f /etc/logstash/conf.d/apache-grok.conf

不用grok原因:
1.grok是非常影响性能的  2.不灵活。除非你懂ruby。 3.生产环境用的流程是：logstash->redis<-python->ES (logstash收集到redis,python读取redis并处理数据后写入到ES)
为什么用python脚本来处理，而不用grok处理，因为grok正则处理大量数据时很麻烦，而python处理大量数据是灵活。

#消息队列：rabbitMQ  kafka   redis    
1.用redis来做消息队列，安装redis:
yum install -y redis
2.vim /etc/redis.conf
-------------------------------------
[root@clusterFS-node4-salt conf.d]# grep '^[a-Z]' /etc/redis.conf
bind 192.168.1.31  #改ip，其他默认
protected-mode yes
port 6379
tcp-backlog 511
timeout 0
tcp-keepalive 300
daemonize yes   #改成后台运行，其他默认
supervised no
pidfile /var/run/redis_6379.pid
loglevel notice
logfile /var/log/redis/redis.log
databases 16
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir /var/lib/redis
slave-serve-stale-data yes
slave-read-only yes
repl-diskless-sync no
repl-diskless-sync-delay 5
repl-disable-tcp-nodelay no
slave-priority 100
appendonly no
appendfilename "appendonly.aof"
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
aof-load-truncated yes
lua-time-limit 5000
slowlog-log-slower-than 10000
slowlog-max-len 128
latency-monitor-threshold 0
notify-keyspace-events ""
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
list-compress-depth 0
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
hll-sparse-max-bytes 3000
activerehashing yes
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit slave 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
hz 10
aof-rewrite-incremental-fsync yes
-------------------------------------
3.启动redis：
systemctl start redis  
4.在logstash中配置redis.conf
[root@clusterFS-node4-salt conf.d]# cat redis.conf
input{
        stdin{}
}

output{
        redis {
                host => "192.168.1.31"
                port => "6379"
                db => "6"
                data_type => "list"
                key => "demo"
        }
}
5.连接操作redis:
[root@clusterFS-node4-salt conf.d]# redis-cli -h 192.168.1.31 -p 6379
---------------
192.168.1.31:6379> info  #查看redis信息
# Keyspace   #没有显示key信息，所以还没有key
192.168.1.31:6379> set name ljdfldjsflsd  #设置一个key
192.168.1.31:6379> info  #再次查看redis信息
# Keyspace
db0:keys=1,expires=0,avg_ttl=0  #显示有key了
192.168.1.31:6379> get name   #获得key名字
"ljdfldjsflsd"
192.168.1.31:6379[6]> keys *   #在db0中获取所有key
1) "demo"

[root@clusterFS-node4-salt conf.d]# /opt/logstash/bin/logstash -f redis.conf  #logstash执行redis.conf，标准输入会输出到redis中，key为demo,demo的类型为list
Settings: Default pipeline workers: 2
Pipeline main started
fdsjfdsljlf
flsdjfldsjfkds
jldsfjlsdflsd
afdsafdsfsafsd


192.168.1.31:6379> info   #redis再次查看所有信息
# Keyspace
db0:keys=1,expires=0,avg_ttl=0
db6:keys=1,expires=0,avg_ttl=0   #增加了db6，并且多了一个key
192.168.1.31:6379> SELECT 6  #进入db6
OK
192.168.1.31:6379[6]> keys *   #查看db6的所有key
1) "demo"
192.168.1.31:6379[6]> type demo  #查看key的类型
list
192.168.1.31:6379[6]> llen demo  #list length 列出key的长度
(integer) 4
192.168.1.31:6379[6]> LINDEX demo -1  #列出key为demo的-1号索引
"{\"message\":\"afdsafdsfsafsd\",\"@version\":\"1\",\"@timestamp\":\"2019-03-03T14:38:25.448Z\",\"host\":\"clusterFS-node4-salt\"}"
--------------
6.将apache的访问日志写入到redis,编写apache.conf：

7.运行apache.conf文件
[root@clusterFS-node4-salt conf.d]# /opt/logstash/bin/logstash -f apache.conf
Settings: Default pipeline workers: 2
Pipeline main started

8.访问http://192.168.1.31:81/触发apache的access-log日志
9.在redis中查看并管理db6的key:
192.168.1.31:6379[6]> keys *
1) "demo" 
2) "apache-accesslog"    #由于触发了access-log日志，所以将日志写到redis中而产生了一个key
192.168.1.31:6379[6]> llen apache-accesslog
(integer) 2
192.168.1.31:6379[6]> lindex apache-accesslog -1    #查看访问apache的用户信息
"{\"message\":\"192.168.1.5 - - [03/Mar/2019:22:57:14 +0800] \\\"GET /favicon.ico HTTP/1.1\\\" 404 209 \\\"-\\\" \\\"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:65.0) Gecko/20100101 Firefox/65.0\\\"\",\"@version\":\"1\",\"@timestamp\":\"2019-03-03T14:57:14.465Z\",\"path\":\"/var/log/httpd/access_log\",\"host\":\"clusterFS-node4-salt\",\"type\":\"apache-access-log\"}"

10.在另外一个节点启动一个logstash，读取redis，详细看流程：
流程：   192.168.1.31  写=>   192.168.1.31的redis    <= 读     192.168.1.37
注意：读看官方文档input插件，写看官方文档output插件
11.在192.168.1.37中编写indexer.conf:
[root@clusterFS-node3-salt conf.d]# vim indexer.conf
input {
         redis {
                host => "192.168.1.31"
                port => "6379"
                db => "6"
                data_type => "list"
                key => "apache-accesslog"
        }
}

output{
        stdout{
                codec => rubydebug
        }
}
12.运行indexer.conf：
[root@clusterFS-node3-salt conf.d]# /opt/logstash/bin/logstash -f indexer.conf #此时会从redis读取并标准输出
Settings: Default pipeline workers: 2
Pipeline main started
{
       "message" => "192.168.1.5 - - [03/Mar/2019:                                   .1\" 200 54 \"-\" \"Mozilla/5.0 (Windows NT 6.1; W                                   0101 Firefox/65.0\"",
      "@version" => "1",
    "@timestamp" => "2019-03-03T14:57:14.464Z",
          "path" => "/var/log/httpd/access_log",
          "host" => "clusterFS-node4-salt",
          "type" => "apache-access-log"
}
{
       "message" => "192.168.1.5 - - [03/Mar/2019:                                   .ico HTTP/1.1\" 404 209 \"-\" \"Mozilla/5.0 (Windo                                   ) Gecko/20100101 Firefox/65.0\"",
      "@version" => "1",
    "@timestamp" => "2019-03-03T14:57:14.465Z",
          "path" => "/var/log/httpd/access_log",
          "host" => "clusterFS-node4-salt",
          "type" => "apache-access-log"
}
13.查看redis读取情况：
192.168.1.31:6379[6]> llen apache-accesslog
(integer) 0   #redis数据全部被读取完了，所以为0
14.再对indexer.conf进行过滤，使从redis读取的数据变成json格式易于管理：
[root@clusterFS-node3-salt conf.d]# vim indexer.conf
input {
         redis {
                host => "192.168.1.31"
                port => "6379"
                db => "6"
                data_type => "list"
                key => "apache-accesslog"
        }
}

filter {
        grok{
                match => { "message" => "%{COMBINEDAPACHELOG}" } #增加gork自带的正则表达式变量进行过滤
        }
}

output{
        stdout{
                codec => rubydebug
        }
}

15.192.168.1.31运行apache.conf，先让apache访问日志写入到redis
/opt/logstash/bin/logstash -f apache.conf
16.192.168.1.37上运行indexer.conf，再让logstash读取redis上的日志并进行过虑标准输出。
/opt/logstash/bin/logstash -f indexer.conf
17.设置logstash读取redis日志到elasticsearch中：
[root@clusterFS-node3-salt ~]# cat /etc/logstash/conf.d/indexer.conf 
input {
         redis {
                host => "192.168.1.31"
                port => "6379"
                db => "6"
                data_type => "list"
                key => "apache-accesslog"
        }
}

filter {
        grok{
                match => { "message" => "%{COMBINEDAPACHELOG}" }
        }
}


output{
        elasticsearch {
            hosts => ["192.168.1.31:9200"]
            index => "apache-accesslog-%{+YYYY.MM}"
        }
}
18.[root@clusterFS-node3-salt conf.d]# /opt/logstash/bin/logstash -f indexer.conf 
19.然后测试ab -n 1000 -c 1 http://192.168.1.31:81/ 
20.在kibana中设置时间显示的时候点开设置时间时左边有个自动刷新功能，一般设成1分钟。
</pre>

#ELKStack生产环境实战
<pre>
需求分析：
访问日志：apache访问日志、nginx访问日志、tomcat访问日志    
错误日志：error日志、java日志(需要使用多行插件并配合正则表达式来处理)
系统日志：/var/log/*   syslog   
运行日志：程序写的。
网络日志：防火墙、交换机、路由器的日志。

学习的插件：file（codec => "json"）、syslog、tcp、grok、redis  

1. 标准化：日志放哪里（/data/logs/），格式是什么(要求JSON)，命名规则（access-log、error_log、runtime_log三个目录），日志怎么切割（按天、按小时。access_log和error_log用crontab进行切分，runtime_log由程序直接行写的）
#所有原始的文本----rsync到NAS后，删除最近三天前的。（不建议复制到NAS和NFS，建议复制到本地，access-log每小时进行切片，error-log进行每天切片。）
2. 工具化：如何使用logstash进行收集方案

注意：1.systemctl restart logstash #由于启动logstash而有些日志没有写到输出的地方，原因是有些日志文件logstash这个用户没有权限读取。2.type是关键字，开发不能用，否则会覆盖你的type类型而无法判断。3.写入redis的时候所有的访问日志设为db6，错误日志写成db7，给它分开。

流程图：
源日志==>logstash收集==>写入redis存储<==logstash读取redis写到es==>kibana展示

###实战：
-----------------------------
#192.168.1.31上：
1. [root@clusterFS-node4-salt conf.d]# cat shipper.conf 
input {
        file{
                path => "/var/log/httpd/access_log"
                start_position => "beginning"
                type => "apache-accesslog"
        }
        file {
                path => "/var/log/elasticsearch/myes.log"
                type => "es-log"
                start_position => "beginning"
                codec => multiline{
                        pattern => "^\["
                        negate => true
                        what => "previous"
        }
    }
}

output{
        if [type] == "apache-accesslog" {
                redis {
                        host => "192.168.1.31"
                        port => "6379"
                        db => "6"
                        data_type => "list"
                        key => "apache-accesslog"
                }
        }
        if [type] == "es-log" {
                redis {
                        host => "192.168.1.31"
                        port => "6379"
                        db => "6"
                        data_type => "list"
                        key => "es-log"
                }
        }
}
2. [root@clusterFS-node4-salt logstash]# vim /etc/init.d/logstash 
LS_USER=root   #将logstash改成root(如果不启端口用root，启用端口用logstash用户)
LS_GROUP=root
3. [root@clusterFS-node4-salt conf.d]# systemctl restart logstash.service 
4. 设置系统日志传送到192.168.1.37：
[root@clusterFS-node4-salt conf.d]# vim /etc/rsyslog.conf 
*.* @@192.168.1.37:514  #配置最后面设置
5. [root@clusterFS-node4-salt conf.d]# systemctl restart rsyslog.service 
-----------------------------
#192.168.1.37上：
1. [root@clusterFS-node3-salt conf.d]# cat indexer.conf 
input {
        syslog{
                type => "system-syslog"
                port => 514
        }
        redis {
                type => "apache-accesslog"
                host => "192.168.1.31"
                port => "6379"
                db => "6"
                data_type => "list"
                key => "apache-accesslog"
        }
        redis {
                type => "es-log"
                host => "192.168.1.31"
                port => "6379"
                db => "6"
                data_type => "list"
                key => "es-log"
        }
}

filter {
        if [type] == "apache-accesslog"{
                grok{
                        match => { "message" => "%{COMBINEDAPACHELOG}" }
                }
        }
}


output{
        if [type] == "apache-accesslog"{
                elasticsearch {
                        hosts => ["192.168.1.31:9200"]
                        index => "apa-accesslog-%{+YYYY.MM}"
                }
        }
        if [type] == "system-syslog"{
                elasticsearch {
                        hosts => ["192.168.1.31:9200"]
                        index => "syslog-%{+YYYY.MM}"
                }
        }
        if [type] == "es-log"{
                elasticsearch {
                        hosts => ["192.168.1.31:9200"]
                        index => "myes-log-%{+YYYY.MM}"
                }
        }
}
2. [root@clusterFS-node3-salt logstash]# vim /etc/init.d/logstash 
LS_USER=root   #将logstash改成root(如果不启端口用root，启用端口用logstash用户)
LS_GROUP=root
3. [root@clusterFS-node3-salt logstash]# systemctl restart logstash.service 
-----------------------------
结论：如果redis list 作为ELKStack消息队列，那么请对所有list key的长度进行监控
llen key_name
根据实际情况，例如超过10万就报警。如果不做redis满了的话就会删除老的数据从而丢失数据。

作业：
消息队列换成kafka
深入学习：在infoQ上搜索kafka
实践==>深度实践：数据写入hadop(web-hdfs写入到hadop)

带着目标学Python:
目标一：
1. 把我们资产的Excel导出来，存放到redis里面。使用hash类型。
2. 使用Python脚本，遍历所有主机，通过zabbix api判断是否增加监控。

目标二：
1. 给所有资产的Excel。增加一个字段，叫做角色。例如nginx、haproxy、php
2. 同目标一的1。然后编写python脚本，调用saltstack api。对该角色执行对应的状态。

目标三：
尝试把现在的Excel。写入到mysql数据库。通过Python导入。
把之前读取redis的操作，改成mysql。

Head First Python  #这本python书要看一下

</pre>


#源码安装ELKstack
<pre>
备注:环境为centos7.6 ，请按实际需求更改
[root@elk down]# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.0.tar.gz #下载elasticsearch
[root@elk down]# wget https://artifacts.elastic.co/downloads/kibana/kibana-6.3.0-linux-x86_64.tar.gz  #下载kibana
[root@elk down]# wget https://artifacts.elastic.co/downloads/logstash/logstash-6.3.0.tar.gz #下载logstash
[root@elk down]# wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.3.0-linux-x86_64.tar.gz #下载filebeat
[root@elk down]# ls
elasticsearch-6.3.0.tar.gz          kibana-6.3.0-linux-x86_64.tar.gz
filebeat-6.3.0-linux-x86_64.tar.gz  logstash-6.3.0.tar.gz
[root@elk down]# tar xf elasticsearch-6.3.0.tar.gz 
[root@elk down]# tar xf kibana-6.3.0-linux-x86_64.tar.gz 
[root@elk down]# tar xf filebeat-6.3.0-linux-x86_64.tar.gz 
[root@elk down]# tar xf logstash-6.3.0.tar.gz 
[root@elk down]# java -version #安装java1.8
java version "1.8.0"
Java(TM) SE Runtime Environment (build 1.8.0-b132)
Java HotSpot(TM) 64-Bit Server VM (build 25.0-b70, mixed mode)
#安装elasticsearch
[root@elk down]# mv elasticsearch-6.3.0 /usr/local/
[root@elk down]# cd /usr/local/elasticsearch-6.3.0/
[root@elk elasticsearch-6.3.0]# grep '^[a-Z]' config/elasticsearch.yml  #编辑elasticsearch配置文件
cluster.name: elkserver
node.name: node-1
path.data: /data
path.logs: /var/log/elasticsearch
network.host: 192.168.1.237
http.port: 9200
bootstrap.memory_lock: true
[root@elk elasticsearch-6.3.0]# mkdir /var/log/elasticsearch -p
[root@elk elasticsearch-6.3.0]# mkdir /data -p
[root@elk elasticsearch-6.3.0]# chown -R elasticsearch.elasticsearch  /var/log/elasticsearch 
[root@elk elasticsearch-6.3.0]# chown -R elasticsearch.elasticsearch  /data 
[root@elk elasticsearch-6.3.0]# groupadd elasticsearch  #新建elasticsearch用户，因为启动elasticsearch不允许root用户启动
[root@elk elasticsearch-6.3.0]# useradd elasticsearch -g elasticsearch -p elasticsearch
[root@elk elasticsearch-6.3.0]#  chown -R elasticsearch.elasticsearch /usr/local/elasticsearch-6.3.0/
[root@elk elasticsearch-6.3.0]# vim /etc/security/limits.conf #更改配置使elasticsearch能运行
elasticsearch soft memlock unlimited  #设置elasticsearch用户软链接，内存锁定大小无限制
elasticsearch hard memlock unlimited
elasticsearch soft nofile 65536  #设置elasticsearch用户软链接，打开文件最大65536个
elasticsearch hard nofile 65536
elasticsearch soft nproc 4096  #设置elasticsearch用户软链接，打开线程最大4096个
elasticsearch hard nproc 4096
备注： elsearch为用户名 可以是使用*进行通配  
nofile 最大打开文件数目
nproc 最大打开进程数目
[root@elk elasticsearch-6.3.0]# vim /etc/sysctl.conf #设置最大的虚拟内存映射数
vm.max_map_count = 262144
[root@elk elasticsearch-6.3.0]# sysctl -p
vm.max_map_count = 262144
[elasticsearch@elk ~]$ /usr/local/elasticsearch-6.3.0/bin/elasticsearch -d
[elasticsearch@elk ~]$ netstat -tunlp | grep 9200
tcp6       0      0 192.168.1.237:9200      :::*                    LISTEN      7414/java 
[elasticsearch@elk elasticsearch-6.3.0]$ curl -i -XGET "http://192.168.1.237:9200/_count" #测试
HTTP/1.1 200 OK
content-type: application/json; charset=UTF-8
content-length: 71

{"count":0,"_shards":{"total":0,"successful":0,"skipped":0,"failed":0}}
###elasticsearch启动脚本
---------------------
[root@elk init.d]# cat elasticsearch 
#!/bin/bash
#
#init file for elasticsearch
#chkconfig: - 86 14
#description: elasticsearch shell
#
#processname: elasticsearch

. /etc/rc.d/init.d/functions

export JAVA_HOME=/usr/local/jdk
export JAVA_BIN=$JAVA_HOME/bin
export PATH=$PATH:$JAVA_BIN
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export JAVA_HOME JAVA_BIN PATH CLASSPATH

##Default variables
USER=elasticsearch
ELSHOME=/usr/local/elasticsearch

case "$1" in
start)
    su $USER<<!
    cd $ELSHOME
    ./bin/elasticsearch -d
!
    echo "elasticsearch startup"
    ;;  
stop)
    es_pid=`ps aux|grep elasticsearch | grep -v 'grep elasticsearch' | awk '{print $2}'`
    kill -9 $es_pid
    echo "elasticsearch stopped"
    ;;  
restart)
    es_pid=`ps aux|grep elasticsearch | grep -v 'grep elasticsearch' | awk '{print $2}'`
    kill -9 $es_pid
    echo "elasticsearch stopped"
    su $USER<<!
    cd $ELSHOME
    ./bin/elasticsearch -d
!
    echo "elasticsearch startup"
    ;;  
*)
    echo "start|stop|restart"
    ;;  
esac

exit $?
---------------------


#安装kibana
[root@elk data]# cd /down/
[root@elk down]# mv kibana-6.3.0-linux-x86_64 /usr/local/
[root@elk down]# cd /usr/local/kibana-6.3.0-linux-x86_64/
[root@elk kibana-6.3.0-linux-x86_64]# vim config/kibana.yml 
[root@elk kibana-6.3.0-linux-x86_64]# grep '^[a-Z]' config/kibana.yml #编辑kibana配置文件
server.port: 5601
server.host: "192.168.1.237"
elasticsearch.url: "http://localhost:9200"
[root@elk kibana-6.3.0-linux-x86_64]# /usr/local/kibana-6.3.0-linux-x86_64/bin/kibana >& /dev/null & #启动kibana
[root@elk ~]# netstat -tunlp | grep 5601
tcp        0      0 192.168.1.237:5601      0.0.0.0:*               LISTEN      8050/node  
[root@elk translations]# wget https://raw.githubusercontent.com/anbai-inc/Kibana_Hanization/master/translations/zh-CN.json #下载汉化kibanajson文件
[root@elk translations]# mvzh-CN.json /usr/local/kibana-6.3.0-linux-x86_64/src/core_plugins/kibana/translations/zh-CN.json #移到此位置
[root@elk kibana]# vim /usr/local/kibana-6.3.0-linux-x86_64/config/kibana.yml 
i18n.defaultLocale: "zh-CN" #修改默认语言
[root@elk translations]# /usr/local/kibana-6.3.0-linux-x86_64/bin/kibana >& /dev/null & #重新启动kibana
#注：当kibana打开时禁止登录表示kibana未成功连接elasticsearch。kibana可以用root用户启动

####kibana启动脚本
----------------
[root@elk init.d]# cat kibana 
#!/bin/bash
#
#init file for kibana 
#chkconfig: - 87 13
#description: kibana shell
#

. /etc/rc.d/init.d/functions

export JAVA_HOME=/usr/local/jdk
export JAVA_BIN=$JAVA_HOME/bin
export PATH=$PATH:$JAVA_BIN
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export JAVA_HOME JAVA_BIN PATH CLASSPATH

##Default variables
ETVAL=0
prog="/usr/local/kibana-6.3.0-linux-x86_64/bin/kibana >& /dev/null &"
desc="kibana"
lockfile="/var/lock/subsys/kibana"

start(){
        echo -n $"Starting $desc:"
        daemon $prog 
        RETVAL=$?
        echo 
        [ $RETVAL -eq 0 ] && touch $lockfile
        return $RETVAL
}

stop(){
        kb_pid=`ps aux|grep '/usr/local/kibana-6.3.0-linux-x86_64/bin'| grep -v 'grep --color=auto /usr/local/kibana-6.3.0-linux-x86_64/bin' | awk '{print $2}'`
        kill -9 $kb_pid
        RETVAL=$?
        echo
        [ $RETVAL -eq 0 ] && rm -f $lockfile
        return $RETVAL
}

restart (){
        stop
        start
}

case "$1" in 
        start)
                start;;
        stop)
                stop;;
        restart)
                restart;;
        *)
                echo $"Usage: $0 {start|stop|restart}"
                RETVAL=1;;
esac
exit #RETVAL
----------------

#安装logstash
[root@elk down]# mv logstash-6.3.0 /usr/local/ 
[root@elk logstash-6.3.0]# vim /etc/profile.d/jdk.sh 
#!/bin/bash
#
export JAVA_HOME=/usr/local/jdk   #加一行JAVA_HOME的变量
export PATH=$PATH:/usr/local/jdk/bin
[root@elk logstash-6.3.0]# . /etc/profile.d/jdk.sh  #生效变量JAVA_HOME
[root@elk logstash-6.3.0]# vim config/damo.conf  #新建一个配置文件
input {
  file {
    path => ["/var/log/messages", "/var/log/sucure"]
    type => "system237-log"
    start_position => "beginning"
  }
  file {
    path => "/var/log/httpd/*log"
    type => "httpd-log"
    start_position => "beginning"
  }
}

filter {
}

output {
  if [type] == "system237-log" {
    elasticsearch {
      hosts => ["192.168.1.237:9200"]
      index => "system-log-%{+YYYY.MM.dd}"
    }
  }
  if [type] == "httpd-log" {
    elasticsearch {
      hosts => ["192.168.1.237:9200"]
      index => "httpd-log-%{+YYYY.MM.dd}"
    }
  }
}

添加索引httpd-log-*，下一步选择时间戳格式进行保存索引

###logstash设置启动脚本
[root@elk bin]# /usr/local/logstash/bin/system-install /usr/local/logstash/config/startup.options systemd
#利用logstash启动脚本安装功能进行注册生成启动脚本，systemd和sysv风格二选一


##ELK日志收集系统使用架构
前端：nginx反向代理+身份验证+ACL
node1: elasticsearch、kibana
node2: elasticsearch、kibana
被收集机器：logstash,收集日志文件发送到elasticsearch
#注：它们之间全靠elasticsearch存储，各elasticsearch节点之间同步数据达到集群的作用


##docker部署elk
[root@elk2 ~]# docker pull elasticsearch:2.4.6
[root@elk2 ~]# docker pull kibana:4.5 
[root@elk2 ~]# docker pull logstash:2.4.0 
##部署elasticsearch
[root@elk2 elk]# tree elasticsearch/
elasticsearch/
├── config
│?? ├── elasticsearch.yml
│?? ├── logging.yml
│?? └── scripts
├── data
│?? └── elkserver
│??     └── nodes
│??         └── 0
│??             ├── node.lock
│??             └── _state
│??                 └── global-0.st
└── logs
[root@elk2 elk]# cat elasticsearch/config/elasticsearch.yml 
cluster.name: elkserver
node.name: node-1
path.data: /usr/share/elasticsearch/data
path.logs: /usr/share/elasticsearch/logs
network.host: 0.0.0.0
http.port: 9200
bootstrap.memory_lock: true

docker run --name els -p 9200:9200 -p 9300:9300 -v /docker/elk/elasticsearch/data:/usr/share/elasticsearch/data -v /docker/elk/elasticsearch/config:/usr/share/elasticsearch/config -v /docker/elk/elasticsearch/logs:/usr/share/elasticsearch/logs elasticsearch:2.4.6

#部署kibana
[root@elk2 elk]# tree kibana/
kibana/
└── config
    └── kibana.yml
[root@elk2 config]# grep '^[a-Z]' kibana.yml 
server.port: 5601
server.host: '0.0.0.0'
elasticsearch.url: 'http://elasticsearch:9200'

[root@elk2 config]# docker run -d --name kb --link els:elasticsearch -v /docker/elk/kibana/config:/etc/kibana -p 5601:5601 kibana:4.5

#部署logstash
[root@elk2 elk]# tree logstash/
logstash/
├── config
│?? └── logstash.conf
└── docker-entrypoint.sh
[root@elk2 logstash]# cat config/logstash.conf 
input {
        tcp {
                port => 5000
        }
}
output {
        elasticsearch {
                hosts => "elasticsearch:9200"
                index => "system-log-%{+YYYY.MM.dd}"
        }
}
[root@elk2 logstash]# cat docker-entrypoint.sh 
#!/bin/bash
/opt/logstash/bin/logstash -f /etc/logstash/conf.d

[root@elk2 elasticsearsh]# docker run --name lg -d -p 5000:5000 -v /docker/elk/logstash/config:/etc/logstash/conf.d -v /docker/elk/logstash/docker-entrypoint.sh:/docker-entrypoint.sh --link els:elasticsearch logstash:2.4.0 

#注：--link els:elasticsearch表示连接名称为els的容器，并取一个elasticsearch别名，使在新建的容器中可以解析els主机的ip
</pre>


#ELK docker部署
<pre>
#1.宿主机系统调优
变更 /etc/security/limits.conf 文件，为其追加以下内容：
* soft nofile 204800
* hard nofile 204800
* soft nproc 204800
* hard nproc 204800
* soft memlock unlimited
* hard memlock unlimited
跳转到 /etc/security/limits.d 目录下，修改相应的 conf 文件，为其追加以下内容：
* soft nproc unlimited
* hard nproc unlimited
除了上述操作以外，还需要变更内核参数，执行以下命令即可：
echo "vm.max_map_count=262144" >> /etc/sysctl.conf
sysctl -p
注意：ERROR: [1] bootstrap checks failed #此错误是宿主机没有进行系统调优导致的

#2.节点部署，凡是集群，必须是3台及以上，这样方便少数服从多数选举
##node1##
[root@node1 elk]# cat docker-compose.yml
version: '3'
services:
  elasticsearch:                    # 服务名称
    image: elasticsearch:7.1.1      # 使用的镜像
    container_name: elasticsearch7.1.1   # 容器名称
    restart: always                 # 失败自动重启策略
    environment:                                    
      - node.name=node-201                   # 节点名称，集群模式下每个节点名称唯一
      - network.publish_host=192.168.43.201  # 用于集群内各机器间通信,其他机器访问本机器的es服务
      - network.host=0.0.0.0                # 设置绑定的ip地址，可以是ipv4或ipv6的，默认为0.0.0.0，
      - discovery.seed_hosts=192.168.43.204,192.168.43.201,192.168.43.202          # es7.x 之后新增的配置，写入候选主节点的设备地址，在开启服务后可以被选为主节点
      - cluster.initial_master_nodes=192.168.43.204,192.168.43.201,192.168.43.202 # es7.x 之后新增的配置，初始化一个新的集群时需要此配置来选举master
      - cluster.name=es-cluster     # 集群名称，相同名称为一个集群
      - bootstrap.memory_lock=true  # 内存交换的选项，官网建议为true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"    # 设置内存
    ulimits:             
      memlock:
        soft: -1      
        hard: -1
    volumes:
      - /data/elk/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml  # 将容器中es的配置文件映射到本地，设置跨域， 否则head插件无法连接该节点
      - esdata:/usr/share/elasticsearch/data  # 存放数据的文件， 注意：这里的esdata为 顶级volumes下的一项。
    ports:
      - 9200:9200    # http端口
      - 9300:9300    # es节点直接交互的端口，非http
  kibana:
    image: docker.elastic.co/kibana/kibana:7.1.1
    container_name: kibana7.1.1
    environment:
      - elasticsearch.hosts=http://elasticsearch:9200  #设置连接的es节点
    hostname: kibana
    depends_on:
      - elasticsearch   #依赖es服务，会先启动es容器在启动kibana
    restart: always
    volumes:
      - /data/elk/kibana.yml:/usr/share/kibana/config/kibana.yml
    ports:
      - 5601:5601 #对外访问端口
  logstash:
    image: docker.elastic.co/logstash/logstash:7.1.1
    container_name: logstash7.1.1
    hostname: logstash
    restart: always
    volumes:
      - /data/elk/test_port.conf:/usr/share/logstash/pipeline/test_port.conf
    depends_on:
      - elasticsearch
    ports:
      - 6666:6666        #这个端口是tcp测试端口
      - 9600:9600		 #这个端口是logstash API端口
      - 5044:5044        #从filebeat读取消息输出到logstash的端口
volumes:
  esdata:
    driver: local    # 会生成一个对应的目录和文件，用来持久化数据，第一次从容器保存到盘，第二次从盘读配置到容器。

[root@node1 elk]# cat elasticsearch.yml 
network.host: 0.0.0.0
http.cors.enabled: true      # 设置跨域，主要用于head插件访问es
http.cors.allow-origin: "*"  # 允许所有域名访问

[root@node1 elk]# cat /data/elk/kibana.yml 
#
# ** THIS IS AN AUTO-GENERATED FILE **
#

# Default Kibana configuration for docker target
server.name: kibana
server.host: "0"
elasticsearch.hosts: [ "http://elasticsearch:9200" ]
xpack.monitoring.ui.container.elasticsearch.enabled: true
i18n.locale: "zh-CN"

[root@node1 elk]# cat /data/elk/test_port.conf 
input {
  tcp {
    type => "tcp"
    port => 6666
    mode => "server"
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "system-test2-%{+YYYY.MM.dd}"
  }
  stdout {
    codec => rubydebug
  }
}

##node2##
[root@node2 elk]# cat docker-compose.yml
version: '3'
services:
  elasticsearch:                    # 服务名称
    image: elasticsearch:7.1.1      # 使用的镜像
    container_name: elasticsearch7.1.1   # 容器名称
    restart: always                 # 失败自动重启策略
    environment:                                    
      - node.name=node-202                   # 节点名称，集群模式下每个节点名称唯一
      - network.publish_host=192.168.43.202  # 用于集群内各机器间通信,其他机器访问本机器的es服务
      - network.host=0.0.0.0                # 设置绑定的ip地址，可以是ipv4或ipv6的，默认为0.0.0.0，
      - discovery.seed_hosts=192.168.43.204,192.168.43.202,192.168.43.201          # es7.x 之后新增的配置，写入候选主节点的设备地址，在开启服务后可以被选为主节点
      - cluster.initial_master_nodes=192.168.43.204,192.168.43.202,192.168.43.201 # es7.x 之后新增的配置，初始化一个新的集群时需要此配置来选举master
      - cluster.name=es-cluster     # 集群名称，相同名称为一个集群
      - bootstrap.memory_lock=true  # 内存交换的选项，官网建议为true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"    # 设置内存
    ulimits:             
      memlock:
        soft: -1      
        hard: -1
    volumes:
      - /data/elk/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml  # 将容器中es的配置文件映射到本地，设置跨域， 否则head插件无法连接该节点
      - esdata:/usr/share/elasticsearch/data  # 存放数据的文件， 注意：这里的esdata为 顶级volumes下的一项。
    ports:
      - 9200:9200    # http端口
      - 9300:9300    # es节点直接交互的端口，非http
  kibana:
    image: docker.elastic.co/kibana/kibana:7.1.1
    container_name: kibana7.1.1
    environment:
      - elasticsearch.hosts=http://elasticsearch:9200  #设置连接的es节点
    hostname: kibana
    depends_on:
      - elasticsearch   #依赖es服务，会先启动es容器在启动kibana
    restart: always
    volumes:
      - /data/elk/kibana.yml:/usr/share/kibana/config/kibana.yml
    ports:
      - 5601:5601 #对外访问端口
  logstash:
    image: docker.elastic.co/logstash/logstash:7.1.1
    container_name: logstash7.1.1
    hostname: logstash
    restart: always
    volumes:
      - /data/elk/test_port.conf:/usr/share/logstash/pipeline/test_port.conf
    depends_on:
      - elasticsearch
    ports:
      - 6666:6666        #这个端口是tcp测试端口
      - 9600:9600		 #这个端口是logstash API端口
      - 5044:5044        #从filebeat读取消息输出到logstash的端口
volumes:
  esdata:
    driver: local    # 会生成一个对应的目录和文件，如何查看，下面有说明。

[root@node2 elk]# cat elasticsearch.yml 
network.host: 0.0.0.0
http.cors.enabled: true      # 设置跨域，主要用于head插件访问es
http.cors.allow-origin: "*"  # 允许所有域名访问

[root@node2 elk]# cat /data/elk/kibana.yml 
#
# ** THIS IS AN AUTO-GENERATED FILE **
#

# Default Kibana configuration for docker target
server.name: kibana
server.host: "0"
elasticsearch.hosts: [ "http://elasticsearch:9200" ]
xpack.monitoring.ui.container.elasticsearch.enabled: true
i18n.locale: "zh-CN"

[root@node2 elk]# cat /data/elk/test_port.conf 
input {
  tcp {
    type => "tcp"
    port => 6666
    mode => "server"
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "system-test2-%{+YYYY.MM.dd}"
  }
  stdout {
    codec => rubydebug
  }
}

##node3##
[root@node3 elk]# cat docker-compose.yml
version: '3'
services:
  elasticsearch:                    # 服务名称
    image: elasticsearch:7.1.1      # 使用的镜像
    container_name: elasticsearch7.1.1   # 容器名称
    restart: always                 # 失败自动重启策略
    environment:                                    
      - node.name=node-203                   # 节点名称，集群模式下每个节点名称唯一
      - network.publish_host=192.168.43.204  # 用于集群内各机器间通信,其他机器访问本机器的es服务
      - network.host=0.0.0.0                # 设置绑定的ip地址，可以是ipv4或ipv6的，默认为0.0.0.0，
      - discovery.seed_hosts=192.168.43.204,192.168.43.202,192.168.43.201          # es7.x 之后新增的配置，写入候选主节点的设备地址，在开启服务后可以被选为主节点
      - cluster.initial_master_nodes=192.168.43.204,192.168.43.202,192.168.43.201 # es7.x 之后新增的配置，初始化一个新的集群时需要此配置来选举master
      - cluster.name=es-cluster     # 集群名称，相同名称为一个集群
      - bootstrap.memory_lock=true  # 内存交换的选项，官网建议为true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"    # 设置内存
    ulimits:             
      memlock:
        soft: -1      
        hard: -1
    volumes:
      - /data/elk/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml  # 将容器中es的配置文件映射到本地，设置跨域， 否则head插件无法连接该节点
      - esdata:/usr/share/elasticsearch/data  # 存放数据的文件， 注意：这里的esdata为 顶级volumes下的一项。
    ports:
      - 9200:9200    # http端口
      - 9300:9300    # es节点直接交互的端口，非http
volumes:
  esdata:
    driver: local    # 会生成一个对应的目录和文件，如何查看，下面有说明。

#3.运行docker:
[root@node1 elk]# docker-compose up -d 
[root@node2 elk]# docker-compose up -d 
[root@node3 elk]# docker-compose up -d 
注：进行docker配置的时候，复制配置文件后就要修改每个主机上的ip及名称，否则会导致集群失败

#4.安装elasticsearch插件HEAD,然后进行查看Elasticsearch状态，
[root@node1 elk]# docker run -d --name es_admin -p 9100:9100 --restart=always mobz/elasticsearch-head:5
#然后访问http://192.168.43.201:9100就可以连接elasticsearch查看状态了

#4.设置分片：
一般以（节点数*1.5或3倍）来计算，比如有4个节点，分片数量一般是6个到12个，每个分片一般分配一个副本
----新建或修改默认模版all_default_template: 新生成的索引都会自动匹配该模版，number_of_shards: 索引分片为5，副本默认也是5，order:优先级，数字越高等级越高
PUT /_template/all_default_template
{
  "index_patterns": ["*"],
  "order" : 100,
  "settings": {
    "number_of_shards": 5
  }
}
----预设置索引，设置分片和副本（用来提前设置索引并设置分片，以防未来会使用,已经存在的索引不能更改）
PUT /testindex
{
   "settings" : {
      "number_of_shards" : 6,
      "number_of_replicas" : 1
   }
注意：在/usr/share/elasticsearch/config/elasticsearch.yml配置文件中#action.destructive_requires_name : true #设置禁用_all和*通配符
PUT /_cluster/settings
{
    "persistent" : {
       "action.destructive_requires_name":true }
}
get /_cluster/settings
例：
1.设置默认索引
PUT /_template/all_default_template
{
  "index_patterns": "*",
  "order" : 100,
  "settings": {
    "number_of_shards": 6,
    "number_of_replicas": "1"
  }
}
GET /_template/all_default_template
2.查看创建的模板，索引模板查看分片是否设置成功
get /_template?pretty=true
3.创建一个索引查看索引主分片和副本分片
put /test
4.查看创建的索引test主分片及副本分片数设置
get /test/_settings?pretty=true
get /_settings?pretty=true    --查看所有索引设置
5.删除索引
delete /test
delete /test*
delete /_all   --删除所有索引，超级危险
6.常用API命令：
get /_cat/nodes
192.168.13.160 74 83 1 0.05 0.16 0.21 dilm - dlog-01
192.168.13.162 58 96 2 0.13 0.19 0.22 dilm * dlog-03
192.168.13.161 55 82 2 0.47 0.41 0.27 dilm - dlog-02
get /_cat/health
1606550453 08:00:53 dlog green 3 3 84 39 0 0 0 0 - 100.0%
/_cat/indices
green open .kibana_1                6w8WSB8VQwyz8SPCPf7r9Q 1 1     38    5 137.4kb  49.5kb



#5.用logstash测试端口 6666进行写入elasticsear测试:
[root@node2 elk]# echo 'hello_world' | ncat 192.168.43.201 6666 #发送消息到logstash端口，从而写入数据到elasticsearch
注意：logstash在命令行运行命令时，会很慢，需要等会(在logstash容器中测试时遇到此问题)
或
#6.用filebeat写入elasticsearch
--安装及配置
[root@node2 log]# sudo rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
[root@node2 elk]# vim /etc/yum.repos.d/filebeat.repo
[elastic-7.x]
name=Elastic repository for 7.x packages
baseurl=https://artifacts.elastic.co/packages/7.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
[root@node2 log]# sudo yum install filebeat
[root@node2 log]# sudo systemctl enable filebeat
[root@node2 log]# egrep -v '#|^$' /etc/filebeat/filebeat.yml 
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/messages
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: true
setup.template.settings:
  index.number_of_shards: 1
setup.kibana:
output.elasticsearch:
  hosts: ["127.0.0.1:9200"]
#output.logstash:	#下面是写入logstash.然后由logstash写入到elasticsearch,疑问见后面其他问题
  #hosts: ["192.168.43.201:5044"]
processors:
  - add_host_metadata: ~
  - add_cloud_metadata: ~
  - add_docker_metadata: ~
  - add_kubernetes_metadata: ~
[root@node2 elk]# filebeat -c /etc/filebeat/filebeat.yml #运行filebeat进行测试
[root@node2 elk]# systemctl start filebeat.service  #以守护进程运行filebeat
[root@node2 log]# logger testtt

----filebeat将日志为Json格式的日志解析成单个字段-----
[root@master /data/elasticsearch]# grep -Ev '#|^$' /etc/filebeat/filebeat.yml 
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/httpd/access_log
  tags: ["httpd"]
  json.keys_under_root: true
  json.overwrite_keys: true
- type: log
  enabled: false
  paths:
    - /var/lib/docker/containers/*/*.log
  tags: ["docker"]
  json.keys_under_root: true
  json.overwrite_keys: true
output.elasticsearch:
  hosts: ["192.168.15.199:7200"]
  indices:
    - index: "docker_%{+yyyy.MM.dd}"
      when.contains:
        tags: "docker"
    - index: "httpd_%{+yyyy.MM.dd}"
      when.contains:
        tags: "httpd"
  username: "jack"
  password: "123456"
-------------------------------------------------


#7.访问kibana,http://192.168.43.201:5601 || http://192.168.43.202:5601
在设置中建立索引模式 system-test* 。就可以在首页查看测试日志了

##其它问题：##
1.需要删除太久远的日志，用来腾出空间
curl -X DELETE http://192.168.43.201:9200/system-test-2020.01.01 #system-test-2020.01.01就是索引名称，system-test-2020*，也可以用通配符来删除
2.elasticsearch需要进行安全防护，不然任何人都可以进行增删查改elasticsearch,很危险
3.kibana索引模式创建好后保存在elasticsearch的数据存储目录中
4.kibana索引模式创建保存时最后一步完成后，在已保存的对象中看不到索引模式，这个问题应该是测试elk时没有对旧的elasticsearch的data数据完全清除导致的
5.当filebeat 的5044端口和logstash tcp6666端口两种方式写入同一个索引时，哪个先写入那么这个索引就只能先写入的用，另外一个则写入不了
6.一般是使用filebeat对系统日志进行收集，然后通过5044端口写入到logstash，然后由logstash的filter模块进行过虑，使系统日志JSON化，最后写入到elasticsearch
7.logstash写入数据时可以将一条日志写入整个es集群所有节点，在logstash配置文件是指定es地址是列表格式，写入数据后在kibana中查找日志时不会是多少日志，只是一条日志。
8.在kibana的配置文件中，可以连接多个es集群。指定es地址是列表格式
--当es7.1.1集群是两个节点的时候：其中一个节点挂了，你往另外一个节点写入json日志时，kibana和es无法读取索引，必须等待另一个节点起来后，两个节点都是正常状态才能写入，之前写入的json日志也不会丢失，还是会在es中，只是未写入排队状态。（两个节点可以保证数据的安全，但不能保证业务的可靠）
--当大于等于3个节点时：不会遇到这个问题。因为当其中一个节点挂掉时，其它任一个节点会选举为master,此时被写入索引不会被锁定，可以正常写入（三个及以上节点及可以保证数据安全也可以保证业务可靠）

######Elasticsearch日志备份：采用快照方式，官方建议
Elasticsearch 做备份有两种方式，一是将数据导出成文本文件，比如通过 elasticdump、esm 等工具将存储在 Elasticsearch 中的数据导出到文件中。二是以备份 elasticsearch data 目录中文件的形式来做快照，也就是 Elasticsearch 中 snapshot 接口实现的功能。第一种方式相对简单，在数据量小的时候比较实用，当应对大数据量场景效率就大打折扣。
###第一种方式备份：
6.4版本docker-compose.yml
-------------
version: '3'
services:
  elasticsearch:                    # 服务名称
    image: elasticsearch:6.4.0      # 使用的镜像
    container_name: elasticsearch6.4.0   # 容器名称
    restart: always                 # 失败自动重启策略
    environment:                                    
      - cluster.name=es-cluster     # 集群名称，相同名称为一个集群
      - bootstrap.memory_lock=true  # 内存交换的选项，官网建议为true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"    # 设置内存
    ulimits:             
      memlock:
        soft: -1      
        hard: -1
    volumes:
      - esdata2:/usr/share/elasticsearch/data  # 存放数据的文件， 注意：这里的esdata为 顶级volumes下的一项。
    ports:
      - 9200:9200    # http端口
      - 9300:9300    # es节点直接交互的端口，非http
  kibana:
    image: docker.elastic.co/kibana/kibana:6.4.0
    container_name: kibana6.4.0
    environment:
      - elasticsearch.hosts=http://elasticsearch:9200  #设置连接的es节点
    hostname: kibana
    depends_on:
      - elasticsearch   #依赖es服务，会先启动es容器在启动kibana
    restart: always
    ports:
      - 5601:5601 #对外访问端口
volumes:
  esdata2:
    driver: local  
-------------------
blog:
#通过指定索引，指定匹配模式，指定时间来查看数据，match_all可以换成match来匹配特定字符
GET /homsom_log/_search
{
  "query": {
    "bool": {
      "must": [
        {
          "match_all": {}
        }
      ],
      "filter": {
        "range": {
          "TimeStamp": {
            "gte":"2020-04-27T00:00:00.000+0800",
            "lt":"2020-04-28T00:00:00.000+0800"
          }
        }
      }
    }
  },
  "sort": [ { "TimeStamp": { "order": "desc" }}],
  "size": 100
}
#通过指定索引，指定匹配模式，指定时间来删除数据
#默认情况下，_delete_by_query使用1000个滚动批次。你可以用scroll_size URL参数来改变批大小
POST /homsom_log/_delete_by_query?scroll_size=5000
{
  "query": {
    "bool": {
      "must": [
        {
          "match_all": {}
        }
      ],
      "filter": {
        "range": {
          "@timestamp": {
            "gte":"2020-04-28T00:00:00.000+0800",
            "lt":"2020-04-28T00:00:00.000+0800"
          }
        }
      }
    }
  }
}
或者
#通过时间查找
GET /homsom_log/_search
{
  "query": {
        "range": {
          "TimeStamp": {
            "time_zone": "+08:00",
            "gte":"2020-04-27T00:00:00.000+0800",
            "lt":"2020-04-28T00:00:00.000+0800"
        }
      }
  }
}
#通过时间来删除
#默认情况下，_delete_by_query使用1000个滚动批次。你可以用scroll_size URL参数来改变批大小
POST /homsom_log/_delete_by_query?scroll_size=5000
{
  "query": {
        "range": {
          "@timestamp": {
            "time_zone": "+08:00",
            "gte":"2020-04-28T00:00:00.000+0800",
            "lt":"2020-04-28T00:00:00.000+0800"
        }
      }
  }
}
例：备份blog20200427-20200428的日志
注：备份的es版本和恢复的es版本必须保持高度一致，否则恢复会失败，恢复时选项可填limit,searchBody等选项限制。具体使用见：https://github.com/taskrabbit/elasticsearch-dump
1.备份mapping
docker run --rm -it -v /dockerdata/elasticsearchdump:/tmp taskrabbit/elasticsearch-dump  --input=http://192.168.13.239:9200/homsom_log  --output=/tmp/homsom_log_mapping.json   --type=mapping
2.备份data
docker run --rm -it -v /dockerdata/elasticsearchdump:/tmp taskrabbit/elasticsearch-dump  --input=http://192.168.13.239:9200/homsom_log  --output=/tmp/homsom_log.json   --type=data --limit=1000 --searchBody='{"query":{"range":{"TimeStamp":{"gte":"2020-04-27T00:00:00.000+0800","lt":"2020-04-28T00:00:00.000+0800"}}}}'
3.恢复mapping
docker run --rm -it -v /dockerdata/elasticsearchdump:/tmp taskrabbit/elasticsearch-dump  --input=/tmp/homsom_log_mapping.json --output=http://172.168.2.222:9200/homsom_log --type=mapping
4.恢复data
docker run --rm -it -v /dockerdata/elasticsearchdump:/tmp taskrabbit/elasticsearch-dump  --input=/tmp/homsom_log.json --output=http://172.168.2.222:9200/homsom_log --type=data --limit=1000

备份hlog操作流程
1.备份mapping
docker run --rm -it -v /mnt:/tmp taskrabbit/elasticsearch-dump  --input=http://192.168.13.237:9200/clog  --output=/tmp/clog--mapping.json   --type=mapping
2.备份data
docker run --rm -it -v /mnt:/tmp taskrabbit/elasticsearch-dump  --input=http://192.168.13.237:9200/clog  --output=/tmp/clog-low20190115.json   --type=data --limit=1000 --searchBody='{"query":{"range":{"time":{"lt":"2019-01-15T00:00:00.000+0800"}}}}'
3. 恢复mapping
docker run --rm -it -v /mnt:/tmp taskrabbit/elasticsearch-dump  --input=/tmp/clog--mapping.json --output=http://172.168.2.223:9200/backup-clog --type=mapping
4.恢复data
docker run --rm -it -v /mnt:/tmp taskrabbit/elasticsearch-dump  --input=/tmp/clog-low20190115.json --output=http://172.168.2.223:9200/backup-clog --type=data --limit=10000

备份blog操作流程:
1.备份mapping
docker run --rm -it -v /mnt:/tmp taskrabbit/elasticsearch-dump  --input=http://192.168.13.239:9200/homsom_log  --output=/tmp/homsom_log--mapping.json   --type=mapping
2.备份data
docker run --rm -it -v /mnt:/tmp taskrabbit/elasticsearch-dump  --input=http://192.168.13.239:9200/homsom_log  --output=/tmp/homsom_log-low20200430.json   --type=data --limit=1000 --searchBody='{"query":{"range":{"TimeStamp":{"lt":"2020-04-30T00:00:00.000+0800"}}}}'
3. 恢复mapping
docker run --rm -it -v /mnt:/tmp taskrabbit/elasticsearch-dump  --input=/tmp/homsom_log--mapping.json --output=http://172.168.2.223:9200/backup-homsom_log  --type=mapping
4.恢复data
docker run --rm -it -v /mnt:/tmp taskrabbit/elasticsearch-dump  --input=/tmp/homsom_log-low20200430.json --output=http://172.168.2.223:9200/backup-homsom_log --type=data --limit=20000

备份hlog操作流程--20200507
GET /clog/_search
{
  "query": {
        "range": {
          "time": {
            "time_zone": "+08:00",
            "gte":"2019-01-15T00:00:00.000",
            "lt":"2019-01-21T00:00:00.000"
        }
      }
  },
  "sort": [ { "time": { "order": "desc" }}],
  "size": 100
}

----20190115-20190120总共218288条记录，备份耗时大概5分钟
备份data
docker run --rm -it -v /mnt:/tmp taskrabbit/elasticsearch-dump  --input=http://192.168.13.237:9200/clog  --output=/tmp/clog-20190115-20190120.json   --type=data --limit=1000 --searchBody='{"query":{"range":{"time":{"time_zone":"+08:00","gte":"2019-01-15T00:00:00.000","lt":"2019-01-21T00:00:00.000"}}}}'
----20190115-20190120总共218288条记录，恢复耗时大概3分钟
恢复data
docker run --rm -it -v /mnt:/tmp taskrabbit/elasticsearch-dump  --input=/tmp/clog-20190115-20190120.json --output=http://172.168.2.223:9200/backup-clog --type=data --limit=10000
----20190121-20190220总共684224条记录，备份耗时大概12分钟
备份data
docker run --rm -it -v /mnt:/tmp taskrabbit/elasticsearch-dump  --input=http://192.168.13.237:9200/clog  --output=/tmp/clog-20190121-20190220.json   --type=data --limit=1000 --searchBody='{"query":{"range":{"time":{"time_zone":"+08:00","gte":"2019-01-21T00:00:00.000","lt":"2019-02-21T00:00:00.000"}}}}'

----20190115-20190120总共684224条记录，恢复大概6分钟
恢复data
docker run --rm -it -v /mnt:/tmp taskrabbit/elasticsearch-dump  --input=/tmp/clog-20190121-20190220.json --output=http://172.168.2.223:9200/backup-clog --type=data --limit=10000
----注：如果恢复数据时报错：blocked by: [FORBIDDEN/12/index read-only / allow delete (api)，则表示磁盘空间不够用，es当磁盘的使用率超过95%时，为了防止节点耗尽磁盘空间，自动将索引设置为只读模式，只需腾出空间，然后手动把节点只读模式设为null即可。--当你从查询删除时这个状态为只读时也是无法删除的
GET /backup-clog/_settings
PUT /backup-clog/_settings
{
  "index.blocks.read_only_allow_delete": null
}
POST /backup-clog/_delete_by_query?scroll_size=5000
{
  "query": {
        "range": {
          "time": {
            "time_zone": "+08:00",
            "lt":"2019-01-12T00:00:00.000"
        }
      }
  }
}

删除hlog小于20200101的旧日志-----20200508操作，一个月一个月来
--查看指定时间日志大小
GET /clog/_search
{
  "query": {
        "range": {
          "time": {
            "time_zone": "+08:00",
            "lt":"2020-01-01T00:00:00.000"
        }
      }
  },
  "sort": [ { "time": { "order": "desc" }}],
  "size": 10
}
--删除指定时间日志
POST /clog/_delete_by_query?scroll_size=5000
{
  "query": {
        "range": {
          "time": {
            "time_zone": "+08:00",
            "lt":"2020-01-01T00:00:00.000"
        }
      }
  }
}
刷新索引---不是必须操作
http://192.168.13.237:9200/clog/_refresh
查看clog的segments---不是必须操作
http://192.168.13.237:9200/_cat/segments/clog
--合并段腾出空间
POST /clog/_forcemerge?only_expunge_deletes=true&max_num_segments=1

###第二种备份的方式，即 snapshot api 的使用
elk日志备份步骤：
yum install -y nfs-utils rpcbind
#vim /etc/exports
  /backup	*(rw,async)
mkdir -p /backup
chown -R 1000:1000 /backup  #elasticsearch UID
systemctl start rpcbind
systemctl start nfs
systemctl enable rcpbind
systemctl enable nfs
exportfs -arv
mount -t nfs 192.168.43.201:/backup /mnt
---共享文件系统存储库（"type": "fs"）使用共享文件系统存储快照。为了注册共享文件系统存储库，必须将相同的共享文件系统安装到所有主节点和数据节点上的相同位置。path.repo 必须在所有主节点和数据节点上的设置中注册。而且更改后必须要重启es服务才行,滚动重启es服务
path.repo: ["/mount/backups"]
path.repo: ["\\\\MY_SERVER\\Snapshots"]
path.repo: ["/mnt"]  --追加到elasticsearch.yml中，这个路径要挂载宿主机上的NFS，NFS权限必须是1000:1000,因为elasticsearch要写入
注：经过实践得出，备份后的快照都会在主节点上显示
1.建仓库
###
PUT /_snapshot/my_backup
{
  "type": "fs",
  "settings": {
    "location": "/mnt"
  }
}
---
{
  "acknowledged" : true
}
###
GET /_snapshot/my_backup
----
{
  "my_backup" : {
    "type" : "fs",
    "settings" : {
      "location" : "/mnt"
    }
  }
}
###备份索引1
PUT /_snapshot/my_backup/snapshot_1?wait_for_completion=true
{
  "indices": "test-2020.04.15",
  "ignore_unavailable": true,
  "include_global_state": false,
  "metadata": {
    "taken_by": "jack",
    "taken_because": "20200405 backup index 'test-2020.04.15' "
  }
}
---
{
  "snapshot" : {
    "snapshot" : "snapshot_1",
    "uuid" : "YP0HygikQdGhuh1zeuQcAA",
    "version_id" : 7010199,
    "version" : "7.1.1",
    "indices" : [
      "test-2020.04.15"
    ],
    "include_global_state" : false,
    "state" : "SUCCESS",
    "start_time" : "2020-04-15T07:42:01.036Z",
    "start_time_in_millis" : 1586936521036,
    "end_time" : "2020-04-15T07:42:01.171Z",
    "end_time_in_millis" : 1586936521171,
    "duration_in_millis" : 135,
    "failures" : [ ],
    "shards" : {
      "total" : 1,
      "failed" : 0,
      "successful" : 1
    }
  }
}
###
GET /_snapshot/my_backup/snapshot_1  --查看快照备份成功与否信息
---
{
  "snapshots" : [
    {
      "snapshot" : "snapshot_1",
      "uuid" : "YP0HygikQdGhuh1zeuQcAA",
      "version_id" : 7010199,
      "version" : "7.1.1",
      "indices" : [
        "test-2020.04.15"
      ],
      "include_global_state" : false,
      "state" : "SUCCESS",
      "start_time" : "2020-04-15T07:42:01.036Z",
      "start_time_in_millis" : 1586936521036,
      "end_time" : "2020-04-15T07:42:01.171Z",
      "end_time_in_millis" : 1586936521171,
      "duration_in_millis" : 135,
      "failures" : [ ],
      "shards" : {
        "total" : 1,
        "failed" : 0,
        "successful" : 1
      }
    }
  ]
}
###备份索引2
PUT /_snapshot/my_backup/snapshot_2?wait_for_completion=true
{
  "indices": "system*",
  "ignore_unavailable": false,
  "include_global_state": false,
  "metadata": {
    "taken_by": "jack",
    "taken_because": "20200405 backup index 'system-jack-2020.04.15' "
  }
}
---
{
  "snapshot" : {
    "snapshot" : "snapshot_2",
    "uuid" : "i3RclQ1NS8uhRh6usJyWnQ",
    "version_id" : 7010199,
    "version" : "7.1.1",
    "indices" : [
      "system-jack-2020.04.15"
    ],
    "include_global_state" : false,
    "state" : "SUCCESS",
    "start_time" : "2020-04-15T07:45:27.308Z",
    "start_time_in_millis" : 1586936727308,
    "end_time" : "2020-04-15T07:45:27.375Z",
    "end_time_in_millis" : 1586936727375,
    "duration_in_millis" : 67,
    "failures" : [ ],
    "shards" : {
      "total" : 1,
      "failed" : 0,
      "successful" : 1
    }
  }
}
###备份索引3
PUT /_snapshot/my_backup/snapshot_3
{
  "indices": "*",
  "ignore_unavailable": true,    #其设置为true将会导致快照创建期间不存在的索引被忽略
  "include_global_state": false,
  "metadata": {
    "taken_by": "jack",
    "taken_because": "20200405 backup index all "
  }
}
---
{
  "accepted" : true
}
###删除一个索引
DELETE /test-2020.04.15
{
  "acknowledged" : true
}
###恢复一个快照
POST /_snapshot/my_backup/snapshot_1/_restore
---
{
  "acknowledged" : true
}
注：当你的快照复制到别的地方时，在恢复的时候需要先建一个仓库，仓库名随便。快照名从你建的快照文件中获取（index-*开头的文件，包括的索引名称也在里面）
###恢复一个快照中指定的索引
POST /_snapshot/my_backup/snapshot_3/_restore
{
  "indices": "system*",    #普通匹配索引
  "ignore_unavailable": false,
  "include_global_state": false,
  "rename_pattern": "index_(.+)",   #正则匹配索引，
  "rename_replacement": "restored_index_$1"  #匹配到的索引重命名
}
---
{
  "accepted" : true
}
###恢复指定索引并重命名
POST /_snapshot/my_backup/snapshot_3/_restore
{
  "indices": "system*",   #这个值不能为空，与下面匹配索引保持一致即可
  "ignore_unavailable": true,
  "include_global_state": false,
  "rename_pattern": "system(.+)",
  "rename_replacement": "restored_index_$1"
}
注：默认情况下，如果一个或多个参与该操作的索引没有所有可用分片的快照，则整个还原操作将失败。例如，如果某些分片无法快照，则会发生这种情况。但仍可以通过设置来恢复这些指数partial来true。请注意，在这种情况下，只有成功快照的分片将被还原，所有丢失的分片将被重新创建为空。
---
{
  "accepted" : true
}
###
POST /_snapshot/my_backup/snapshot_3/_restore?wait_for_completion=true
{
  "indices": "system*",
  "ignore_unavailable": true,
  "include_global_state": false,
  "rename_pattern": "system(.+)",
  "rename_replacement": "restored_system_$1"
}
---
{
  "snapshot" : {
    "snapshot" : "snapshot_3",
    "indices" : [
      "restored_system_-jack-2020.04.15"
    ],
    "shards" : {
      "total" : 1,
      "failed" : 0,
      "successful" : 1
    }
  }
}
###在还原过程中更改索引设置，可以覆盖大多数索引设置。 -----操作都支持等待完成响应再退出参数wait_for_completion=true
POST /_snapshot/my_backup/snapshot_3/_restore
{
  "indices": "test-2020.04.15",
  "ignore_unavailable": true,
  "index_settings": {
    "index.number_of_replicas": 0   ###表示恢复时不创建副本，不设置此选项默认是按集群节点数创建副本
  },
  "ignore_index_settings": [
    "index.refresh_interval"
  ]
}
---
{
  "accepted" : true
}
###获取当前正在运行的快照及其详细状态信息的列表
GET /_snapshot/_status
###返回给定快照的详细状态信息，即使当前未运行该快照
GET /_snapshot/my_backup/snapshot_1,snapshot_2/_status
###监视快照的还原进度
GET /_snapshot/my_backup/snapshot_1
GET /_snapshot/my_backup/snapshot_1/_status
###停止快照和恢复操作
DELETE /_snapshot/my_backup/snapshot_1

#复制索引并重命名
POST /_reindex
{
  "source": {
    "index": "newdon_2020.11.26"
  },
  "dest": {
    "index": "newdon_jack"
  }
}
</pre>

<pre>
#基于sebp/elk镜像部署elk
--------部署环境---------
node1: 192.168.13.160
node2: 192.168.13.161
node3: 192.168.13.162
------------------------
-------
node1: 192.168.13.160
-------
[root@localhost elasticsearch]# cat /home/dockerdata/elasticsearch/elasticsearch.yml
############cluster#########
node.name: dlog-01 
network.host: 0.0.0.0
http.port: 9200
transport.tcp.port: 9300
path.repo: /var/backups
cluster.name: dlog
network.publish_host: 192.168.13.160
discovery.seed_hosts: ["192.168.13.160:9300","192.168.13.161:9300","192.168.13.162:9300"]
cluster.initial_master_nodes: ["192.168.13.161"]
node.master: true
node.data: true
discovery.zen.minimum_master_nodes: 2
discovery.zen.fd.ping_timeout: 1m
discovery.zen.fd.ping_retries: 5
http.cors.enabled: true    
http.cors.allow-origin: "*" 
############################
-------
[root@localhost elasticsearch]# cat /home/dockerdata/elasticsearch/kibana.yml 
server.name: train_kbn01
server.host: "0.0.0.0"
i18n.locale: "zh-CN"
-------
docker run -d --restart=always --name=dlog01 \
-p 9200:9200 \
-p 9300:9300 \
-p 5601:5601 \
-e ES_CONNECT_RETRY=60 \
-e KIBANA_CONNECT_RETRY=60 \
-e LOGSTASH_START=0 \
-e ELASTICSEARCH_START=1 \
-e KIBANA_START=1 \
-e ES_HEAP_SIZE="2g" \
-e TZ="Asia/Shanghai" \
-v /home/dockerdata/elasticsearch/elasticsearch.yml:/etc/elasticsearch/elasticsearch.yml \
-v /home/dockerdata/elasticsearch/kibana.yml:/opt/kibana/config/kibana.yml \
-v /home/dockerdata/elasticsearch/es_data:/var/lib/elasticsearch \
sebp/elk:761 

-------
node2: 192.168.13.161
-------
[root@redis-slave1 /home/dockerdata/elasticsearch]# cat /home/dockerdata/elasticsearch/elasticsearch.yml
##########cluster#########
node.name: dlog-02 
network.host: 0.0.0.0
http.port: 9200
transport.tcp.port: 9300
path.repo: /var/backups
cluster.name: dlog
network.publish_host: 192.168.13.161
discovery.seed_hosts: ["192.168.13.160:9300","192.168.13.161:9300","192.168.13.162:9300"]
cluster.initial_master_nodes: ["192.168.13.160"]
node.master: true
node.data: true
discovery.zen.minimum_master_nodes: 1
discovery.zen.fd.ping_timeout: 1m
discovery.zen.fd.ping_retries: 5
http.cors.enabled: true    
http.cors.allow-origin: "*" 
##########################
-------
[root@redis-slave1 /home/dockerdata/elasticsearch]# cat /home/dockerdata/elasticsearch/kibana.yml 
server.name: train_kbn01
server.host: "0.0.0.0"
i18n.locale: "zh-CN"
-------
docker run -d --restart=always --name=dlog02 \
-p 9200:9200 \
-p 9300:9300 \
-p 5601:5601 \
-e ES_CONNECT_RETRY=60 \
-e KIBANA_CONNECT_RETRY=60 \
-e LOGSTASH_START=0 \
-e ELASTICSEARCH_START=1 \
-e KIBANA_START=1 \
-e ES_HEAP_SIZE="2g" \
-e TZ="Asia/Shanghai" \
-v /home/dockerdata/elasticsearch/elasticsearch.yml:/etc/elasticsearch/elasticsearch.yml \
-v /home/dockerdata/elasticsearch/kibana.yml:/opt/kibana/config/kibana.yml \
-v /home/dockerdata/elasticsearch/es_data:/var/lib/elasticsearch \
sebp/elk:761 

-------
node3: 192.168.13.162
-------
[root@redis1_s2 elasticsearch]# cat /home/dockerdata/elasticsearch/elasticsearch.yml
###########cluster#########
node.name: dlog-03 
network.host: 0.0.0.0
http.port: 9200
transport.tcp.port: 9300
path.repo: /var/backups
cluster.name: dlog
network.publish_host: 192.168.13.162
discovery.seed_hosts: ["192.168.13.160:9300","192.168.13.161:9300","192.168.13.162:9300"]
cluster.initial_master_nodes: ["192.168.13.160"]
node.master: true
node.data: true
discovery.zen.minimum_master_nodes: 2
discovery.zen.fd.ping_timeout: 1m
discovery.zen.fd.ping_retries: 5
http.cors.enabled: true    
http.cors.allow-origin: "*" 
###########################
-------
[root@redis1_s2 elasticsearch]# cat /home/dockerdata/elasticsearch/kibana.yml 
server.name: train_kbn02
server.host: "0.0.0.0"
i18n.locale: "zh-CN"
-------
docker run -d --restart=always --name=dlog03 \
-p 9200:9200 \
-p 9300:9300 \
-p 5601:5601 \
-e ES_CONNECT_RETRY=60 \
-e KIBANA_CONNECT_RETRY=60 \
-e LOGSTASH_START=0 \
-e ELASTICSEARCH_START=1 \
-e KIBANA_START=1 \
-e ES_HEAP_SIZE="2g" \
-e TZ="Asia/Shanghai" \
-v /home/dockerdata/elasticsearch/elasticsearch.yml:/etc/elasticsearch/elasticsearch.yml \
-v /home/dockerdata/elasticsearch/kibana.yml:/opt/kibana/config/kibana.yml \
-v /home/dockerdata/elasticsearch/es_data:/var/lib/elasticsearch \
sebp/elk:761 
----------------
#注：elasticsearch.yml配置文件中
cluster.initial_master_nodes：表示节点启动时选择一个节点为master,当集群中的master转移为另一个节点时，则未启动的elasticsearch节点配置文件应将此配置项改为新的master节点地址，否则不会加入已经存在的集群，只会此节点成为一个孤立集群节点。
discovery.seed_hosts: 表示初始集群的各个候选节点地址，后面新加节点也可加入进来
discovery.zen.minimum_master_nodes: 表示最小两个候选节点投票才能选举出一个master，小于两个节点投票则不能选举master(elasticsearch集群将不能正常服务)，公式为(取整)：master数/2+1
----------------
[root@redis1_s2 elasticsearch]# curl -XGET http://192.168.13.161:9200/_cat/nodes?v
ip             heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
192.168.13.161           39          59   1    0.05    0.20     0.43 dilm      *      dlog-02
192.168.13.160           27          85   1    0.07    0.32     0.42 dilm      -      dlog-01
192.168.13.162           30          95   2    0.05    0.15     0.31 dilm      -      dlog-03
----------------
#注：上面master节点为dlog-02,表示后面初始master节点dlog-01转移为dlog-02了，后面的新加入节点cluster.initial_master_nodes应配置为dlog-02
----------------
-----single-----
---
[root@test /data/elk/elasticsearch]# cat elasticsearch.yml 
#elasticsearch user read this file. UID:991
node.name: elk
path.repo: /var/backups
network.host: 0.0.0.0
cluster.initial_master_nodes: ["elk"]
---
[root@test /data/elk]# cat /data/elk/kibana/kibana.yml 
server.name: syslog_kibana
server.host: "0.0.0.0"
i18n.locale: "zh-CN"
-----
docker run -d --restart=always --name=rsyslog  \
-p 9401:9200 \
-p 9402:5601 \
-e ES_CONNECT_RETRY=60 \
-e KIBANA_CONNECT_RETRY=60 \
-e LOGSTASH_START=0 \
-e ELASTICSEARCH_START=1 \
-e KIBANA_START=1 \
-e ES_HEAP_SIZE="2g" \
-e TZ="Asia/Shanghai" \
-v /data/elk/kibana/kibana.yml:/opt/kibana/config/kibana.yml \
-v /data/elk/elasticsearch/elasticsearch.yml:/etc/elasticsearch/elasticsearch.yml \
-v /data/elk/es_data:/var/lib/elasticsearch \
-v /data/elk/es_snapshot:/var/backups \
192.168.13.235:8000/ops/elk:761 
---
[root@test /data/elk]# chown -R 991:991 es_snapshot
----filebeat collect syslog-----
#vim /etc/rsyslog.conf   ----开启udp514端口，设置模板IpTemplate，所以主机日志命名及路径位置，下面引用这些模板，并且发送日志级别:*.*
---
$ModLoad imudp
$UDPServerRun 514
$template IpTemplate,"/var/log/hosts/%FROMHOST-IP%.log" 
*.*  ?IpTemplate 
#### GLOBAL DIRECTIVES ####   ----必须在GLOBAL DIRECTIVES前面开启或增加如上配置
---
[root@test /data/elk]# grep -Ev '#|^$' /etc/filebeat/filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/hosts/192.168.3.1.log
  tags: ["newdon"]
- type: log
  enabled: true
  paths:
    - /usr/local/nginx/logs/access.log
  tags: ["nginx_access_log"]
  json.keys_under_root: true
  json.overwrite_keys: true
- type: log
  enabled: true
  paths:
    - /var/log/hosts/192.168.13.236.log
    - /var/log/hosts/127.0.0.1.log
  tags: ["linux_host"]
  exclude_lines: ["filebeat: 2"]
output.elasticsearch:
  hosts: ["127.0.0.1:9401"]
  indices:
    - index: "newdon_%{+yyyy.MM.dd}"
      when.contains:
        tags: "newdon"
    - index: "linux_host_%{+yyyy.MM.dd}"
      when.contains:
        tags: "linux_host"
  #username: "jack"
  #password: "123456"
---
注：可用nginx进行htpasswd对es进行认证，反向代理es。在本机上配置iptables防火墙，只允许指定主机无密码访问es9200（一般指nginx反向代理），其它主机全部拒绝。如果是本机nginx代理本机docker，则无须进行只允许本机访问这条策略，因为防火墙只对外部有用，对同一个宿主机上的内部通信则不能控制。但是其它主机全部拒绝依旧配置。
---
[root@test /data/elk/nginx]# cat docker-compose-nginx.yml 
version: '3'
services:
  nginx:
    image: nginx:1.19.1
    container_name: nginx
    hostname: nginx
    restart: always
    #network_mode: host
    volumes:
      - /data/elk/nginx/.login.txt:/etc/nginx/.login.txt
      - /data/elk/nginx/default.conf:/etc/nginx/conf.d/default.conf
    ports:
      - 9200:9200
      - 5601:5601
    deploy:
     resources:
        limits:
           cpus: '2'
           memory: 500M
        reservations:
           cpus: '0.5'
           memory: 100M
---
[root@test /data/elk/nginx]# cat default.conf 
server {
    listen       0.0.0.0:5601;
    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
	add_header backendIP $upstream_addr;
	proxy_redirect off;
	proxy_set_header Host $host;
	proxy_set_header X-Real-IP $remote_addr;
	proxy_set_header X-Real-Port $remote_port;
	proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
	proxy_pass http://192.168.13.50:9402;
	auth_basic_user_file /etc/nginx/.login.txt;
	auth_basic	"htpasswd" ;
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
    }
}

server {
    listen       0.0.0.0:9200;
    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
	add_header backendIP $upstream_addr;
	proxy_redirect off;
	proxy_set_header Host $host;
	proxy_set_header X-Real-IP $remote_addr;
	proxy_set_header X-Real-Port $remote_port;
	proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
	proxy_pass http://192.168.13.50:9401;
	auth_basic_user_file /etc/nginx/.login.txt;
	auth_basic	"htpasswd" ;
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
    }
}
server_tokens off;
---
sudo /usr/local/bin/docker-compose -f docker-compose-nginx.yml up -d
[root@test /data/elk/elasticsearch]# cat ../nginx/docker_iptables.sh 
iptables -I FORWARD 1 -o docker0 -p tcp --dport 9200 -j DROP
iptables -I FORWARD 2 -o docker0 -p tcp --dport 5601 -j DROP
---
#其它主机配置/etc/rsyslog.conf,增加所有日志并且级别为info的发送到syslog服务器
*.info 	@192.168.13.50
--------------------------------

#Elasticsearch问题汇总
问题：
Validation Failed: 1: this action would add [8] total shards, but this cluster currently has [999]/[1000] maximum shards open
原因：
Elasticsearch默认分片数量1000
解决：
可以增加分片数量或者取消副本数，这里以设置为3000为例：
curl -X PUT localhost:9401/_cluster/settings -H "Content-Type: application/json" -d '{ "persistent": { "cluster.max_shards_per_node": "3000" } }'


#分词器安装
DOWNLOAD URL: https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.6.1/elasticsearch-analysis-ik-7.6.1.zip
docker cp analysis elasticsearch:/opt/elasticsearch/plugins
docker exec -it elasticsearch /bin/sh
cd /opt/elasticsearch/plugins
chown -R elasticsearch:elasticsearch analysis
chmod -R 755 analysis

</pre>
