#ELKStack
官网：https://www.elastic.co
插件文档：https://www.elastic.co/guide/en/logstash-versioned-plugins/current/index.html
ELKStack简介:
对于日志来说，最常见的需求就是收集、存储、查询、展示，开源社区正好有相对应的开源项目：logstash（收集）、elasticsearch（存储+搜索）、kibana（展示），我们将这三个组合起来的技术称之为ELKStack，所以说ELKStack指的是Elasticsearch、Logstash、Kibana技术栈的结合

Elasticsearch天生是分布式的，有两种方式进行通信：1.组播（加到组中，在组中的主机互相通信） 2.单播(指定主机)

<pre>
安装JDK
[root@clusterFS-node4-salt ~]# yum install -y java-1.8.0
[root@clusterFS-node4-salt ~]# java -version
openjdk version "1.8.0_191"
OpenJDK Runtime Environment (build 1.8.0_191-b12)
OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)
##Elasticsearch部署
Elasticsearch首先需要Java环境，所以需要提前安装好JDK，可以直接使用yum安装。也可以从Oracle官网下载JDK进行安装。开始之前要确保JDK正常安装并且环境变量也配置正确：
YUM安装ElasticSearch
1.下载并安装GPG key:
[root@clusterFS-node4-salt ~]# rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
2.添加yum仓库:
[root@clusterFS-node4-salt ~]# vim /etc/yum.repos.d/elasticsearch.repo
[elasticsearch-2.x]
name=Elasticsearch repository for 2.x packages
baseurl=http://packages.elastic.co/elasticsearch/2.x/centos
gpgcheck=1
gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
enabled=1
3.安装elasticsearch:
[root@clusterFS-node4-salt ~]#  yum install -y elasticsearch
4.更改elasticsearch.yml配置文件：
[root@clusterFS-node4-salt elasticsearch]# vim elasticsearch.yml
[root@clusterFS-node4-salt elasticsearch]# grep '^[a-Z]' elasticsearch.yml
cluster.name: myes
node.name: elk-node1
path.data: /data/es-data
path.logs: /var/log/elasticsearch
bootstrap.memory_lock: true  #锁住物理内存不适用虚拟内存
network.host: 192.168.1.31
http.port: 9200
5.配置/data目录给elasticsearch权限：
[root@clusterFS-node4-salt ~]# chown -R elasticsearch:elasticsearch /data
6.启动elasticsearch:
[root@clusterFS-node4-salt elasticsearch]# systemctl start elasticsearch.service
7.测试elasticsearch服务是否正常：
[root@clusterFS-node4-salt ~]# curl -i -XGET "http://192.168.1.31:9200/_count"
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 59

{"count":0,"_shards":{"total":0,"successful":0,"failed":0}}
注：java使用得是lucene，lucene是搜索巨头，大部分搜索基本都是lucene，lucene本身没有集群，高可用，分片。而elasticsearch具备集群，高可用和分片，elasticsearch底层是lucene。elasticsearch跟glusterFS一样，分布式是无中心的。
8.装插件，用插件联系elasticsearch[也可以用api]:
[root@clusterFS-node4-salt ~]# /usr/share/elasticsearch/bin/plugin install mobz/elasticsearch-head  #直接从github上去抓取安装
[root@clusterFS-node4-salt ~]# /usr/share/elasticsearch/bin/plugin install lmenezes/elasticsearch-kopf  #直接从github上去抓取安装
lukas-vlcek/bigdesk#bigdesk插件直接从github上抓取，但是版本不支持
[root@clusterFS-node4-salt ~]# /usr/share/elasticsearch/bin/plugin install marvel-agent  #从官网上去抓取
9.测试插件kopf:
http://192.168.1.31:9200/_plugin/kopf/ 
10.访问插件head[用于集群管理]:
http://192.168.1.31:9200/_plugin/head/
10.1：
点击复合查询-输入/index-demo/test-选择POST-输入json信息并提交请求
json信息：
{
  "user": "oldboy",
  "mesg": "hello world"
}
输出：
{

    "_index": "index-demo",
    "_type": "test",
    "_id": "AWkvYTZQKeOJs7st2toX",
    "_version": 1,
    "_shards": {
        "total": 2,
        "successful": 1,
        "failed": 0
    },
    "created": true

}
10.2:
点击概览-可查看到0 1 2 3 4 这些分片-深黑色是主分片浅灰色是副本分片-主分片和负分片要部署在不同的机器上-点击连接-集群健康值为黄色为良好，为红色是主负本分片都丢失了
注意：在生产环境中，http://192.168.1.31:9200/_plugin/head/打开要5分钟，因为elasticsearch要去收集日志并整理，要花时间所致。
11.访问插件kopf:
http://192.168.1.31:9200/_plugin/kopf/
12.另外一个节点也要安装elasticsearch,集群名要一样，节点不一样，此时有两个es了，两个节点根据组播查找到另外节点【也可以用单播查找到另外节点】。两个节点互相查找到以后会进行选举，产生主节点和备节点，主节点和备节点对用户来说不重要，随便哪一个节点都可以转发信息。分片来说，只能是主分片等划分，副本分片不行。
13.由于elasticsearch组播无法查询到另外节点，此时用单播方式，[root@clusterFS-node3-salt ~]# vim /etc/elasticsearch/elasticsearch.yml
discovery.zen.ping.unicast.hosts: ["192.168.1.31", "192.168.1.37"]    #ip地址可以加端口。默认是9200
14.[root@clusterFS-node3-salt ~]# systemctl restart elasticsearch.service
15.网页查看http://192.168.1.31:9200/_plugin/head/，此时有两个节点，带"五角星"的为主节点。健康值也为绿色。
16.可以用curl来查看集群的健康状态：curl -XGET http://192.168.1.31:9200/_cluster/health?pretty=tryue
[root@clusterFS-node3-salt .ssh]# curl -XGET http://192.168.1.31:9200/_cluster/health?pretty=true  #?pretty=true是要漂亮的输出
{
  "cluster_name" : "myes",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 5,
  "active_shards" : 10,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
17._cat来查看：
[root@clusterFS-node3-salt .ssh]# curl -XGET http://192.168.1.31:9200/_cat
=^.^=
/_cat/allocation
/_cat/shards
/_cat/shards/{index}
/_cat/master
/_cat/nodes
/_cat/indices
/_cat/indices/{index}
/_cat/segments
/_cat/segments/{index}
/_cat/count
/_cat/count/{index}
/_cat/recovery
/_cat/recovery/{index}
/_cat/health
/_cat/pending_tasks
/_cat/aliases
/_cat/aliases/{alias}
/_cat/thread_pool
/_cat/plugins
/_cat/fielddata
/_cat/fielddata/{fields}
/_cat/nodeattrs
/_cat/repositories
/_cat/snapshots/{repository}
18.[root@clusterFS-node3-salt .ssh]# curl -XGET http://192.168.1.31:9200/_cat/nodes
192.168.1.31 192.168.1.31 7 97 0.14 d * els-node1
192.168.1.37 192.168.1.37 7 96 0.00 d m els-node2
19.生产部署硬件：一般内存64G，JVM不要超过32G，SSD硬盘最好。不要调度。CPU越多越好，网卡越块越好。JVM版本越高越好。
20.[root@clusterFS-node3-salt .ssh]# cat /proc/sys/vm/max_map_count
65530
21.上elasticsearch第一件事情就是改openfile:sysctl -w vm.max_map_count=262144

#logstash部署：
YUM部署LogStash:
1.下载并安装GPG key:
rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
2.添加yum仓库:
vim /etc/yum.repos.d/logstash.repo
[logstash-2.3]
name=Logstash repository for 2.3.x packages
baseurl=https://packages.elastic.co/logstash/2.3/centos
gpgcheck=1
gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearch
enabled=1
3.安装logstash:
yum install -y logstash
4.启动logstash:
systemctl start logstash

测试使用：
1.[root@clusterFS-node3-salt ~]# curl -i -XGET http://192.168.1.31:9200/_cluster/health?pretty=true
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 458

{
  "cluster_name" : "myes",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 5,
  "active_shards" : 10,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
2.[root@clusterFS-node3-salt ~]# /opt/logstash/bin/logstash -e 'input { stdin {} } output { stdout {} }' #从标准输入到标准输出,所以下面直接输出在屏幕上
Settings: Default pipeline workers: 2
Pipeline main started
hello
2019-03-01T08:06:19.817Z clusterFS-node3-salt hello
3.[root@clusterFS-node3-salt ~]# /opt/logstash/bin/logstash -e 'input { stdin {} } output { stdout { codec => rubydebug } }' #使用codec=>rubydebug使输出更美观
Settings: Default pipeline workers: 2
Pipeline main started
aa
{
       "message" => "aa",
      "@version" => "1",
    "@timestamp" => "2019-03-01T08:09:33.108Z",
          "host" => "clusterFS-node3-salt"
}
4.[root@clusterFS-node3-salt ~]# /opt/logstash/bin/logstash -e 'input { stdin {} } output { elasticsearch { hosts=>[ "192.168.1.31:9200" ] index=>"logstash-%{+YYYY.MM.dd}" } }'   #输出到elasticsearch
5.[root@clusterFS-node3-salt ~]# /opt/logstash/bin/logstash -e 'input { stdin {} } output { stdout { codec => rubydebug }  elasticsearch { hosts=>[ "192.168.1.31:9200" ] index=>"logstash-%{+YYYY.MM.dd}" } }'  #输出到elasticsearch中和标准输出
6.logstash脚本放置在/etc/logstash/conf.d/下，因为/etc/init.d/logstash里面LS_CONF_DIR=/etc/logstash/conf.d配置了从这里读。
7.cd /etc/logstash/conf.d && vim demo.conf
input{
    stdin{}
}

filter{
}

output{
##号代表注释
    elasticsearch {
        hosts => ["192.168.1.31:9200"]
        index => "logstash-%{+YYYY.MM.dd}"
    }
    stdout{
        codec => rubydebug
    }
}
8.[root@clusterFS-node3-salt conf.d]# /opt/logstash/bin/logstash -f /etc/logstash/conf.d/demo.conf  #命令运行这个配置文件就可以输出到elasticsearch和logstash中了
9.logstash语法：
1.行 = 事件	2.input  output		3.事件 ->  input -> codec -> filter -> codec -> output(编解码的动作)
10.input模块：
file插件收集系统日志：
input{
    file {
        path => ["/var/log/messages", "/var/log/secure"]
        type => "system-log"
        start_position => "beginning"
    }
}

filter{
}

output{
    elasticsearch {
        hosts => ["192.168.1.31:9200"]
        index => "system-log-%{+YYYY.MM}"
    }
}

#kabana部署:
kabana跟logstash没有一点关系，kabana只是为elasticsearch设置的一个设置界面
Yum安装Kibana
1.下载并安装GPG key
[root@clusterFS-node4-salt log]#  rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
2.添加yum仓库
[root@clusterFS-node4-salt log]# cat /etc/yum.repos.d/kibana.repo
[kibana-4.5]
name=Kibana repository for 4.5.x packages
baseurl=http://packages.elastic.co/kibana/4.5/centos
gpgcheck=1
gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
enabled=1
3.安装kibana
[root@clusterFS-node4-salt log]# yum install -y kibana 
4.编辑kibana配置文件
[root@clusterFS-node4-salt log]# vim /opt/kibana/config/kibana.yml
[root@clusterFS-node4-salt log]# grep '^[a-Z]' /opt/kibana/config/kibana.yml
server.port: 5601
server.host: "0.0.0.0"
elasticsearch.url: "http://192.168.1.31:9200"
kibana.index: ".kibana"
5.http://192.168.1.31:5601打开kibana,进入setting菜单添加index索引，勾上{Use event times to create index names [DEPRECATED] }选项，进入Index name or pattern选项，设置index索引匹配名称[logstash-]YYYY.MM.DD
6.进入discovery菜单进行日志的查看及管理

7.测试java日志提交es-log到新的索引
vim /etc/logstash/conf.d/file.conf
编写配置格式：每行4个空格
input{
    file {
        path => "/var/log/elasticsearch/myes.log"
        type => "es-log"
        start_position => "beginning"
    }
    file {
        path => ["/var/log/secure", "/var/log/messages"]
        type => "system-log"
        start_position => "beginning"
    }
}

filter{
}

output{
    if [type] == "es-log" {    #使用if来判断type
        elasticsearch {
            hosts => ["192.168.1.31:9200"]
            index => "es-log-%{+YYYY.MM}"
        }
    }
    if [type] == "system-log" {
        elasticsearch {
            hosts => ["192.168.1.31:9200"]
            index => "system-log-%{+YYYY.MM}"
        }
    }
}

需求：由于java日志报错exception时有多行是一个事件，而logstash却认为是多行而不是一行，所以要用multiline插件来更改使logstash认为这是一个事件输出为一行
8.测试配置文件：
[root@clusterFS-node4-salt conf.d]# cat codec.conf
input{
    stdin {
        codec => multiline{
            pattern => "^\["
            negate => true
            what => "previous"
        }
    }
}

filter{
}

output{
    stdout {
        codec => rubydebug
    }
}

9.加入正式配置文件：
[root@clusterFS-node4-salt conf.d]# cat file.conf
input{
    file {
        path => "/var/log/elasticsearch/myes.log"
        type => "es-log"
        start_position => "beginning"
        codec => multiline{
            pattern => "^\["   #匹配到[开关的
            negate => true    #为真时
            what => "previous"   #之前的合并
        }

    }
    file {
        path => ["/var/log/secure", "/var/log/messages"]
        type => "system-log"
        start_position => "beginning"
    }
}

filter{
}

output{
    if [type] == "es-log" {
        elasticsearch {
            hosts => ["192.168.1.31:9200"]
            index => "es-log-%{+YYYY.MM}"
        }
    }
    if [type] == "system-log" {
        elasticsearch {
            hosts => ["192.168.1.31:9200"]
            index => "system-log-%{+YYYY.MM}"
        }
    }
}

10.删除~目录下.sincedb开头的隐藏文件（用户记录index索引收集到哪一行的数据），重新收集日志
11.运行/opt/logstash/bin/logstash -f /etc/logstash/conf.d/file.conf重新创建index，并且使java报的Exception都在一行显示，不会错误的显示多行
注意：建立索引必需使用/opt/logstash/bin/logstash -f /etc/logstash/conf.d/file.conf来建立，重启服务并不会建立索引。(由于启动/etc/init.d/logstash restart 而有些日志没有写到输出的地方，原因是有些日志文件logstash这个用户没有权限读取。)

12.收集nginx日志：
logstash使用json插件来收集nginx日志，排版nginx日志取得需要的字段信息。因为nginx日志默认并不是json格式，好在nginx支持日志改成json格式。
例如：
 ab -n 1000 -c 1 http://192.168.1.31/  #-n为请求数据，-c为并发数（concurrency）
less /var/log/nginx/access.log  #查看nginx访问日志，日志格式在/etc/nginx/nginx.conf配置文件里面log_format参数下
 log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                   '$status $body_bytes_sent "$http_referer" '
                   '"$http_user_agent" "$http_x_forwarded_for"';   #前面是系统自带日志参数
实现json格式两种方法：
第一种方式：nginx日志改成json格式:
log_format  access_log_json 	'{"user_ip":"$http_x_real_ip","lan_ip":"$remote_addr","log_time    	":"$time_iso8601","user_req":"$request","http_code":"$status","body_bytes_sent":"	$body_bytes_se    nt","req_time":"$request_time","user_ua":"$http_user_agent"} ';
#access_log_json为自己加进去的json格式
 access_log  /var/log/nginx/access_log_json.log  access_log_json;  #调用access_log_json日志格式 
第二种方式：
文件通过Redis直接收取，然后Python脚本读取Redis,写成Json,写入elasticsearch。
测试json日志格式：
input {
    file {
        path => "/var/log/nginx/access_log_json.log"
        codec => "json"
    }
}

filter {}

output{
    stdout {
        codec => rubydebug
    }
}

加入nginx.conf到/etc/logstash/conf.d/下：
[root@clusterFS-node4-salt conf.d]# cat nginx.conf
input {
    file {
        path => "/var/log/nginx/access_log_json.log"
        codec => "json"
        type => "nginx-access-log"
    }
}

filter {}

output{
    elasticsearch {
        hosts => ["192.168.1.31:9200"]
        index => "nginx-access-log-%{+YYYY.MM.dd}"
    }
}

13./opt/logstash/bin/logstash -f /etc/logstash/conf.d/nginx.conf #加-t是测试配置文件
注：ls -a /var/lib/logstash/  #logstash的sincedb默认在这个路径下
14.kibana中加nginx-access-log的index,之后可以在discovery下使用自己定义的json字段进行排序了。

14.在kibana中添加dashboard仪表盘，有makedown语法的（常用作紧急联系人）、计算器总计、饼图、柱状图、搜索结果等添加到仪表盘中。设置参数时Aggregation选项添加terms、Field 再可以添加自己配置的json键名。
注：当设置图表参数时显示?时，是index添加有问题，删除重新添加即可
生产环境的部署情况：
1.每个ES上面都启动一个Kibana
2.Kibana都连自己的ES
3.前端Nginx做负载均衡（kibana性能到20多个人时就极限）、ip_hast、身份验证（限制访问）、ACL。

##Rsyslog日志(Redhat6之后不叫syslog了) #shadowsocksVPN
syslog插件：logstash开启514端口，其他节点就可以把所有系统日志传到这台主机的514端口了。
##syslog插件 （logstash自带所有插件，免安装）

1.vim /etc/logstash/conf.d/syslog.conf
input {
    syslog{   #意思是开启收集系统日志的端口，用于收取其他节点的系统日志
        type => "system-syslog"
        port => 514
    }

}

output {
    stdout {
        codec => rubydebug
    }
}

2.在需要传送系统日志的机器/etc/rsyslog.conf下添加这行配置
*.* @@192.168.1.31:514  
3.重启rsyslog服务：
systemctl restart rsyslog.service

4.写入到正式配置文件：
[root@clusterFS-node4-salt conf.d]# cat syslog.conf
input {
    syslog{
        type => "system-syslog"
        port => 514
    }

}

output {
    elasticsearch {
        hosts => ["192.168.1.31:9200"]
        index => "system-syslog-%{+YYYY.MM}"   #YYYY.MM前有个+号
    }
    stdout {
        codec => rubydebug
    }
}

#TCP日志（使用tcp插件）
当logstash收集日志时少收了一些日志，要补日志到kibana上，有两种方法：
1. 把缺少日志写到文件，再通过logstash把文件传到kibana
2. 用tcp来传少的日志到kibana(这种方式较灵活)
tcp插件：
1. [root@clusterFS-node4-salt conf.d]# cat tcp.conf
input {
    tcp {   #开启tcp端口收集日志
        type => "tcp"
        port => 6666
        mode => "server"
    }
}

output {
    stdout{
        codec => rubydebug
    }
}
2. /opg/logstash/bin/logstash -f tcp.conf
3. 通过任意一种方式连接端口传送日志到logstash
echo "hehe" | nc 192.168.1.31 6666
nc 192.168.1.31 6666 < /etc/resolv.conf
echo "hello" > /dev/tcp/192.168.1.31/6666
 
##filter模块：
#grok插件：
1. grok系统自带正则表达式目录（自带大部分应用正则表达式）：
[root@clusterFS-node4-salt conf.d]# cd /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-2.0.5/patterns/
2. [root@clusterFS-node4-salt conf.d]# cat grok.conf
input {
        stdin {}
}

filter {
        grok {
                match => { "message" => "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}" }
        }
}

output {
        stdout {
                codec => rubydebug
        }
}
3. /opt/logstash/bin/logstash -f /etc/logstash/conf.d/grok.conf
4. 输入55.3.244.1 GET /index.html 15824 0.043进行测试正则表达式测试的结果。
5. 使用grok自带的正没表达式变量来过滤日志
[root@clusterFS-node4-salt conf.d]# cat apache-grok.conf
input {
        file{
                path => "/var/log/httpd/access_log"
                start_position => "beginning"
                type => "apache-accesslog"
        }
}

filter {
        grok{
                match => { "message" => "%{COMBINEDAPACHELOG}" }  #COMBINEDAPACHELOG为grok自带的正则表达式变量，变量在/opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-2.0.5/patterns/grok-patterns文件中
        }
}

output{
        elasticsearch{
                hosts => "192.168.1.31:9200"
                index => "apache-accesslog-%{+YYYY.MM}"
        }
}

/opt/logstash/bin/logstash -f /etc/logstash/conf.d/apache-grok.conf

不用grok原因:
1.grok是非常影响性能的  2.不灵活。除非你懂ruby。 3.生产环境用的流程是：logstash->redis<-python->ES (logstash收集到redis,python读取redis并处理数据后写入到ES)
为什么用python脚本来处理，而不用grok处理，因为grok正则处理大量数据时很麻烦，而python处理大量数据是灵活。

#消息队列：rabbitMQ  kafka   redis    
1.用redis来做消息队列，安装redis:
yum install -y redis
2.vim /etc/redis.conf
-------------------------------------
[root@clusterFS-node4-salt conf.d]# grep '^[a-Z]' /etc/redis.conf
bind 192.168.1.31  #改ip，其他默认
protected-mode yes
port 6379
tcp-backlog 511
timeout 0
tcp-keepalive 300
daemonize yes   #改成后台运行，其他默认
supervised no
pidfile /var/run/redis_6379.pid
loglevel notice
logfile /var/log/redis/redis.log
databases 16
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir /var/lib/redis
slave-serve-stale-data yes
slave-read-only yes
repl-diskless-sync no
repl-diskless-sync-delay 5
repl-disable-tcp-nodelay no
slave-priority 100
appendonly no
appendfilename "appendonly.aof"
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
aof-load-truncated yes
lua-time-limit 5000
slowlog-log-slower-than 10000
slowlog-max-len 128
latency-monitor-threshold 0
notify-keyspace-events ""
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
list-compress-depth 0
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
hll-sparse-max-bytes 3000
activerehashing yes
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit slave 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
hz 10
aof-rewrite-incremental-fsync yes
-------------------------------------
3.启动redis：
systemctl start redis  
4.在logstash中配置redis.conf
[root@clusterFS-node4-salt conf.d]# cat redis.conf
input{
        stdin{}
}

output{
        redis {
                host => "192.168.1.31"
                port => "6379"
                db => "6"
                data_type => "list"
                key => "demo"
        }
}
5.连接操作redis:
[root@clusterFS-node4-salt conf.d]# redis-cli -h 192.168.1.31 -p 6379
---------------
192.168.1.31:6379> info  #查看redis信息
# Keyspace   #没有显示key信息，所以还没有key
192.168.1.31:6379> set name ljdfldjsflsd  #设置一个key
192.168.1.31:6379> info  #再次查看redis信息
# Keyspace
db0:keys=1,expires=0,avg_ttl=0  #显示有key了
192.168.1.31:6379> get name   #获得key名字
"ljdfldjsflsd"
192.168.1.31:6379[6]> keys *   #在db0中获取所有key
1) "demo"

[root@clusterFS-node4-salt conf.d]# /opt/logstash/bin/logstash -f redis.conf  #logstash执行redis.conf，标准输入会输出到redis中，key为demo,demo的类型为list
Settings: Default pipeline workers: 2
Pipeline main started
fdsjfdsljlf
flsdjfldsjfkds
jldsfjlsdflsd
afdsafdsfsafsd


192.168.1.31:6379> info   #redis再次查看所有信息
# Keyspace
db0:keys=1,expires=0,avg_ttl=0
db6:keys=1,expires=0,avg_ttl=0   #增加了db6，并且多了一个key
192.168.1.31:6379> SELECT 6  #进入db6
OK
192.168.1.31:6379[6]> keys *   #查看db6的所有key
1) "demo"
192.168.1.31:6379[6]> type demo  #查看key的类型
list
192.168.1.31:6379[6]> llen demo  #list length 列出key的长度
(integer) 4
192.168.1.31:6379[6]> LINDEX demo -1  #列出key为demo的-1号索引
"{\"message\":\"afdsafdsfsafsd\",\"@version\":\"1\",\"@timestamp\":\"2019-03-03T14:38:25.448Z\",\"host\":\"clusterFS-node4-salt\"}"
--------------
6.将apache的访问日志写入到redis,编写apache.conf：

7.运行apache.conf文件
[root@clusterFS-node4-salt conf.d]# /opt/logstash/bin/logstash -f apache.conf
Settings: Default pipeline workers: 2
Pipeline main started

8.访问http://192.168.1.31:81/触发apache的access-log日志
9.在redis中查看并管理db6的key:
192.168.1.31:6379[6]> keys *
1) "demo" 
2) "apache-accesslog"    #由于触发了access-log日志，所以将日志写到redis中而产生了一个key
192.168.1.31:6379[6]> llen apache-accesslog
(integer) 2
192.168.1.31:6379[6]> lindex apache-accesslog -1    #查看访问apache的用户信息
"{\"message\":\"192.168.1.5 - - [03/Mar/2019:22:57:14 +0800] \\\"GET /favicon.ico HTTP/1.1\\\" 404 209 \\\"-\\\" \\\"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:65.0) Gecko/20100101 Firefox/65.0\\\"\",\"@version\":\"1\",\"@timestamp\":\"2019-03-03T14:57:14.465Z\",\"path\":\"/var/log/httpd/access_log\",\"host\":\"clusterFS-node4-salt\",\"type\":\"apache-access-log\"}"

10.在另外一个节点启动一个logstash，读取redis，详细看流程：
流程：   192.168.1.31  写=>   192.168.1.31的redis    <= 读     192.168.1.37
注意：读看官方文档input插件，写看官方文档output插件
11.在192.168.1.37中编写indexer.conf:
[root@clusterFS-node3-salt conf.d]# vim indexer.conf
input {
         redis {
                host => "192.168.1.31"
                port => "6379"
                db => "6"
                data_type => "list"
                key => "apache-accesslog"
        }
}

output{
        stdout{
                codec => rubydebug
        }
}
12.运行indexer.conf：
[root@clusterFS-node3-salt conf.d]# /opt/logstash/bin/logstash -f indexer.conf #此时会从redis读取并标准输出
Settings: Default pipeline workers: 2
Pipeline main started
{
       "message" => "192.168.1.5 - - [03/Mar/2019:                                   .1\" 200 54 \"-\" \"Mozilla/5.0 (Windows NT 6.1; W                                   0101 Firefox/65.0\"",
      "@version" => "1",
    "@timestamp" => "2019-03-03T14:57:14.464Z",
          "path" => "/var/log/httpd/access_log",
          "host" => "clusterFS-node4-salt",
          "type" => "apache-access-log"
}
{
       "message" => "192.168.1.5 - - [03/Mar/2019:                                   .ico HTTP/1.1\" 404 209 \"-\" \"Mozilla/5.0 (Windo                                   ) Gecko/20100101 Firefox/65.0\"",
      "@version" => "1",
    "@timestamp" => "2019-03-03T14:57:14.465Z",
          "path" => "/var/log/httpd/access_log",
          "host" => "clusterFS-node4-salt",
          "type" => "apache-access-log"
}
13.查看redis读取情况：
192.168.1.31:6379[6]> llen apache-accesslog
(integer) 0   #redis数据全部被读取完了，所以为0
14.再对indexer.conf进行过滤，使从redis读取的数据变成json格式易于管理：
[root@clusterFS-node3-salt conf.d]# vim indexer.conf
input {
         redis {
                host => "192.168.1.31"
                port => "6379"
                db => "6"
                data_type => "list"
                key => "apache-accesslog"
        }
}

filter {
        grok{
                match => { "message" => "%{COMBINEDAPACHELOG}" } #增加gork自带的正则表达式变量进行过滤
        }
}

output{
        stdout{
                codec => rubydebug
        }
}

15.192.168.1.31运行apache.conf，先让apache访问日志写入到redis
/opt/logstash/bin/logstash -f apache.conf
16.192.168.1.37上运行indexer.conf，再让logstash读取redis上的日志并进行过虑标准输出。
/opt/logstash/bin/logstash -f indexer.conf
17.设置logstash读取redis日志到elasticsearch中：
[root@clusterFS-node3-salt ~]# cat /etc/logstash/conf.d/indexer.conf 
input {
         redis {
                host => "192.168.1.31"
                port => "6379"
                db => "6"
                data_type => "list"
                key => "apache-accesslog"
        }
}

filter {
        grok{
                match => { "message" => "%{COMBINEDAPACHELOG}" }
        }
}


output{
        elasticsearch {
            hosts => ["192.168.1.31:9200"]
            index => "apache-accesslog-%{+YYYY.MM}"
        }
}
18.[root@clusterFS-node3-salt conf.d]# /opt/logstash/bin/logstash -f indexer.conf 
19.然后测试ab -n 1000 -c 1 http://192.168.1.31:81/ 
20.在kibana中设置时间显示的时候点开设置时间时左边有个自动刷新功能，一般设成1分钟。
</pre>

#ELKStack生产环境实战
<pre>
需求分析：
访问日志：apache访问日志、nginx访问日志、tomcat访问日志    
错误日志：error日志、java日志(需要使用多行插件并配合正则表达式来处理)
系统日志：/var/log/*   syslog   
运行日志：程序写的。
网络日志：防火墙、交换机、路由器的日志。

学习的插件：file（codec => "json"）、syslog、tcp、grok、redis  

1. 标准化：日志放哪里（/data/logs/），格式是什么(要求JSON)，命名规则（access-log、error_log、runtime_log三个目录），日志怎么切割（按天、按小时。access_log和error_log用crontab进行切分，runtime_log由程序直接行写的）
#所有原始的文本----rsync到NAS后，删除最近三天前的。（不建议复制到NAS和NFS，建议复制到本地，access-log每小时进行切片，error-log进行每天切片。）
2. 工具化：如何使用logstash进行收集方案

注意：1.systemctl restart logstash #由于启动logstash而有些日志没有写到输出的地方，原因是有些日志文件logstash这个用户没有权限读取。2.type是关键字，开发不能用，否则会覆盖你的type类型而无法判断。3.写入redis的时候所有的访问日志设为db6，错误日志写成db7，给它分开。

流程图：
源日志==>logstash收集==>写入redis存储<==logstash读取redis写到es==>kibana展示

###实战：
-----------------------------
#192.168.1.31上：
1. [root@clusterFS-node4-salt conf.d]# cat shipper.conf 
input {
        file{
                path => "/var/log/httpd/access_log"
                start_position => "beginning"
                type => "apache-accesslog"
        }
        file {
                path => "/var/log/elasticsearch/myes.log"
                type => "es-log"
                start_position => "beginning"
                codec => multiline{
                        pattern => "^\["
                        negate => true
                        what => "previous"
        }
    }
}

output{
        if [type] == "apache-accesslog" {
                redis {
                        host => "192.168.1.31"
                        port => "6379"
                        db => "6"
                        data_type => "list"
                        key => "apache-accesslog"
                }
        }
        if [type] == "es-log" {
                redis {
                        host => "192.168.1.31"
                        port => "6379"
                        db => "6"
                        data_type => "list"
                        key => "es-log"
                }
        }
}
2. [root@clusterFS-node4-salt logstash]# vim /etc/init.d/logstash 
LS_USER=root   #将logstash改成root(如果不启端口用root，启用端口用logstash用户)
LS_GROUP=root
3. [root@clusterFS-node4-salt conf.d]# systemctl restart logstash.service 
4. 设置系统日志传送到192.168.1.37：
[root@clusterFS-node4-salt conf.d]# vim /etc/rsyslog.conf 
*.* @@192.168.1.37:514  #配置最后面设置
5. [root@clusterFS-node4-salt conf.d]# systemctl restart rsyslog.service 
-----------------------------
#192.168.1.37上：
1. [root@clusterFS-node3-salt conf.d]# cat indexer.conf 
input {
        syslog{
                type => "system-syslog"
                port => 514
        }
        redis {
                type => "apache-accesslog"
                host => "192.168.1.31"
                port => "6379"
                db => "6"
                data_type => "list"
                key => "apache-accesslog"
        }
        redis {
                type => "es-log"
                host => "192.168.1.31"
                port => "6379"
                db => "6"
                data_type => "list"
                key => "es-log"
        }
}

filter {
        if [type] == "apache-accesslog"{
                grok{
                        match => { "message" => "%{COMBINEDAPACHELOG}" }
                }
        }
}


output{
        if [type] == "apache-accesslog"{
                elasticsearch {
                        hosts => ["192.168.1.31:9200"]
                        index => "apa-accesslog-%{+YYYY.MM}"
                }
        }
        if [type] == "system-syslog"{
                elasticsearch {
                        hosts => ["192.168.1.31:9200"]
                        index => "syslog-%{+YYYY.MM}"
                }
        }
        if [type] == "es-log"{
                elasticsearch {
                        hosts => ["192.168.1.31:9200"]
                        index => "myes-log-%{+YYYY.MM}"
                }
        }
}
2. [root@clusterFS-node3-salt logstash]# vim /etc/init.d/logstash 
LS_USER=root   #将logstash改成root(如果不启端口用root，启用端口用logstash用户)
LS_GROUP=root
3. [root@clusterFS-node3-salt logstash]# systemctl restart logstash.service 
-----------------------------
结论：如果redis list 作为ELKStack消息队列，那么请对所有list key的长度进行监控
llen key_name
根据实际情况，例如超过10万就报警。如果不做redis满了的话就会删除老的数据从而丢失数据。

作业：
消息队列换成kafka
深入学习：在infoQ上搜索kafka
实践==>深度实践：数据写入hadop(web-hdfs写入到hadop)

带着目标学Python:
目标一：
1. 把我们资产的Excel导出来，存放到redis里面。使用hash类型。
2. 使用Python脚本，遍历所有主机，通过zabbix api判断是否增加监控。

目标二：
1. 给所有资产的Excel。增加一个字段，叫做角色。例如nginx、haproxy、php
2. 同目标一的1。然后编写python脚本，调用saltstack api。对该角色执行对应的状态。

目标三：
尝试把现在的Excel。写入到mysql数据库。通过Python导入。
把之前读取redis的操作，改成mysql。

Head First Python  #这本python书要看一下

</pre>


#源码安装ELKstack
<pre>
备注:环境为centos7.6 ，请按实际需求更改
[root@elk down]# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.0.tar.gz #下载elasticsearch
[root@elk down]# wget https://artifacts.elastic.co/downloads/kibana/kibana-6.3.0-linux-x86_64.tar.gz  #下载kibana
[root@elk down]# wget https://artifacts.elastic.co/downloads/logstash/logstash-6.3.0.tar.gz #下载logstash
[root@elk down]# wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.3.0-linux-x86_64.tar.gz #下载filebeat
[root@elk down]# ls
elasticsearch-6.3.0.tar.gz          kibana-6.3.0-linux-x86_64.tar.gz
filebeat-6.3.0-linux-x86_64.tar.gz  logstash-6.3.0.tar.gz
[root@elk down]# tar xf elasticsearch-6.3.0.tar.gz 
[root@elk down]# tar xf kibana-6.3.0-linux-x86_64.tar.gz 
[root@elk down]# tar xf filebeat-6.3.0-linux-x86_64.tar.gz 
[root@elk down]# tar xf logstash-6.3.0.tar.gz 
[root@elk down]# java -version #安装java1.8
java version "1.8.0"
Java(TM) SE Runtime Environment (build 1.8.0-b132)
Java HotSpot(TM) 64-Bit Server VM (build 25.0-b70, mixed mode)
#安装elasticsearch
[root@elk down]# mv elasticsearch-6.3.0 /usr/local/
[root@elk down]# cd /usr/local/elasticsearch-6.3.0/
[root@elk elasticsearch-6.3.0]# grep '^[a-Z]' config/elasticsearch.yml  #编辑elasticsearch配置文件
cluster.name: elkserver
node.name: node-1
path.data: /data
path.logs: /var/log/elasticsearch
network.host: 192.168.1.237
http.port: 9200
bootstrap.memory_lock: true
[root@elk elasticsearch-6.3.0]# mkdir /var/log/elasticsearch -p
[root@elk elasticsearch-6.3.0]# mkdir /data -p
[root@elk elasticsearch-6.3.0]# chown -R elasticsearch.elasticsearch  /var/log/elasticsearch 
[root@elk elasticsearch-6.3.0]# chown -R elasticsearch.elasticsearch  /data 
[root@elk elasticsearch-6.3.0]# groupadd elasticsearch  #新建elasticsearch用户，因为启动elasticsearch不允许root用户启动
[root@elk elasticsearch-6.3.0]# useradd elasticsearch -g elasticsearch -p elasticsearch
[root@elk elasticsearch-6.3.0]#  chown -R elasticsearch.elasticsearch /usr/local/elasticsearch-6.3.0/
[root@elk elasticsearch-6.3.0]# vim /etc/security/limits.conf #更改配置使elasticsearch能运行
elasticsearch soft memlock unlimited  #设置elasticsearch用户软链接，内存锁定大小无限制
elasticsearch hard memlock unlimited
elasticsearch soft nofile 65536  #设置elasticsearch用户软链接，打开文件最大65536个
elasticsearch hard nofile 65536
elasticsearch soft nproc 4096  #设置elasticsearch用户软链接，打开线程最大4096个
elasticsearch hard nproc 4096
备注： elsearch为用户名 可以是使用*进行通配  
nofile 最大打开文件数目
nproc 最大打开进程数目
[root@elk elasticsearch-6.3.0]# vim /etc/sysctl.conf #设置最大的虚拟内存映射数
vm.max_map_count = 262144
[root@elk elasticsearch-6.3.0]# sysctl -p
vm.max_map_count = 262144
[elasticsearch@elk ~]$ /usr/local/elasticsearch-6.3.0/bin/elasticsearch -d
[elasticsearch@elk ~]$ netstat -tunlp | grep 9200
tcp6       0      0 192.168.1.237:9200      :::*                    LISTEN      7414/java 
[elasticsearch@elk elasticsearch-6.3.0]$ curl -i -XGET "http://192.168.1.237:9200/_count" #测试
HTTP/1.1 200 OK
content-type: application/json; charset=UTF-8
content-length: 71

{"count":0,"_shards":{"total":0,"successful":0,"skipped":0,"failed":0}}
###elasticsearch启动脚本
---------------------
[root@elk init.d]# cat elasticsearch 
#!/bin/bash
#
#init file for elasticsearch
#chkconfig: - 86 14
#description: elasticsearch shell
#
#processname: elasticsearch

. /etc/rc.d/init.d/functions

export JAVA_HOME=/usr/local/jdk
export JAVA_BIN=$JAVA_HOME/bin
export PATH=$PATH:$JAVA_BIN
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export JAVA_HOME JAVA_BIN PATH CLASSPATH

##Default variables
USER=elasticsearch
ELSHOME=/usr/local/elasticsearch

case "$1" in
start)
    su $USER<<!
    cd $ELSHOME
    ./bin/elasticsearch -d
!
    echo "elasticsearch startup"
    ;;  
stop)
    es_pid=`ps aux|grep elasticsearch | grep -v 'grep elasticsearch' | awk '{print $2}'`
    kill -9 $es_pid
    echo "elasticsearch stopped"
    ;;  
restart)
    es_pid=`ps aux|grep elasticsearch | grep -v 'grep elasticsearch' | awk '{print $2}'`
    kill -9 $es_pid
    echo "elasticsearch stopped"
    su $USER<<!
    cd $ELSHOME
    ./bin/elasticsearch -d
!
    echo "elasticsearch startup"
    ;;  
*)
    echo "start|stop|restart"
    ;;  
esac

exit $?
---------------------


#安装kibana
[root@elk data]# cd /down/
[root@elk down]# mv kibana-6.3.0-linux-x86_64 /usr/local/
[root@elk down]# cd /usr/local/kibana-6.3.0-linux-x86_64/
[root@elk kibana-6.3.0-linux-x86_64]# vim config/kibana.yml 
[root@elk kibana-6.3.0-linux-x86_64]# grep '^[a-Z]' config/kibana.yml #编辑kibana配置文件
server.port: 5601
server.host: "192.168.1.237"
elasticsearch.url: "http://localhost:9200"
[root@elk kibana-6.3.0-linux-x86_64]# /usr/local/kibana-6.3.0-linux-x86_64/bin/kibana >& /dev/null & #启动kibana
[root@elk ~]# netstat -tunlp | grep 5601
tcp        0      0 192.168.1.237:5601      0.0.0.0:*               LISTEN      8050/node  
[root@elk translations]# wget https://raw.githubusercontent.com/anbai-inc/Kibana_Hanization/master/translations/zh-CN.json #下载汉化kibanajson文件
[root@elk translations]# mvzh-CN.json /usr/local/kibana-6.3.0-linux-x86_64/src/core_plugins/kibana/translations/zh-CN.json #移到此位置
[root@elk kibana]# vim /usr/local/kibana-6.3.0-linux-x86_64/config/kibana.yml 
i18n.defaultLocale: "zh-CN" #修改默认语言
[root@elk translations]# /usr/local/kibana-6.3.0-linux-x86_64/bin/kibana >& /dev/null & #重新启动kibana
#注：当kibana打开时禁止登录表示kibana未成功连接elasticsearch。kibana可以用root用户启动

####kibana启动脚本
----------------
[root@elk init.d]# cat kibana 
#!/bin/bash
#
#init file for kibana 
#chkconfig: - 87 13
#description: kibana shell
#

. /etc/rc.d/init.d/functions

export JAVA_HOME=/usr/local/jdk
export JAVA_BIN=$JAVA_HOME/bin
export PATH=$PATH:$JAVA_BIN
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export JAVA_HOME JAVA_BIN PATH CLASSPATH

##Default variables
ETVAL=0
prog="/usr/local/kibana-6.3.0-linux-x86_64/bin/kibana >& /dev/null &"
desc="kibana"
lockfile="/var/lock/subsys/kibana"

start(){
        echo -n $"Starting $desc:"
        daemon $prog 
        RETVAL=$?
        echo 
        [ $RETVAL -eq 0 ] && touch $lockfile
        return $RETVAL
}

stop(){
        kb_pid=`ps aux|grep '/usr/local/kibana-6.3.0-linux-x86_64/bin'| grep -v 'grep --color=auto /usr/local/kibana-6.3.0-linux-x86_64/bin' | awk '{print $2}'`
        kill -9 $kb_pid
        RETVAL=$?
        echo
        [ $RETVAL -eq 0 ] && rm -f $lockfile
        return $RETVAL
}

restart (){
        stop
        start
}

case "$1" in 
        start)
                start;;
        stop)
                stop;;
        restart)
                restart;;
        *)
                echo $"Usage: $0 {start|stop|restart}"
                RETVAL=1;;
esac
exit #RETVAL
----------------

#安装logstash
[root@elk down]# mv logstash-6.3.0 /usr/local/ 
[root@elk logstash-6.3.0]# vim /etc/profile.d/jdk.sh 
#!/bin/bash
#
export JAVA_HOME=/usr/local/jdk   #加一行JAVA_HOME的变量
export PATH=$PATH:/usr/local/jdk/bin
[root@elk logstash-6.3.0]# . /etc/profile.d/jdk.sh  #生效变量JAVA_HOME
[root@elk logstash-6.3.0]# vim config/damo.conf  #新建一个配置文件
input {
  file {
    path => ["/var/log/messages", "/var/log/sucure"]
    type => "system237-log"
    start_position => "beginning"
  }
  file {
    path => "/var/log/httpd/*log"
    type => "httpd-log"
    start_position => "beginning"
  }
}

filter {
}

output {
  if [type] == "system237-log" {
    elasticsearch {
      hosts => ["192.168.1.237:9200"]
      index => "system-log-%{+YYYY.MM.dd}"
    }
  }
  if [type] == "httpd-log" {
    elasticsearch {
      hosts => ["192.168.1.237:9200"]
      index => "httpd-log-%{+YYYY.MM.dd}"
    }
  }
}

添加索引httpd-log-*，下一步选择时间戳格式进行保存索引

###logstash设置启动脚本
[root@elk bin]# /usr/local/logstash/bin/system-install /usr/local/logstash/config/startup.options systemd
#利用logstash启动脚本安装功能进行注册生成启动脚本，systemd和sysv风格二选一


##ELK日志收集系统使用架构
前端：nginx反向代理+身份验证+ACL
node1: elasticsearch、kibana
node2: elasticsearch、kibana
被收集机器：logstash,收集日志文件发送到elasticsearch
#注：它们之间全靠elasticsearch存储，各elasticsearch节点之间同步数据达到集群的作用


##docker部署elk
[root@elk2 ~]# docker pull elasticsearch:2.4.6
[root@elk2 ~]# docker pull kibana:4.5 
[root@elk2 ~]# docker pull logstash:2.4.0 
##部署elasticsearch
[root@elk2 elk]# tree elasticsearch/
elasticsearch/
├── config
│?? ├── elasticsearch.yml
│?? ├── logging.yml
│?? └── scripts
├── data
│?? └── elkserver
│??     └── nodes
│??         └── 0
│??             ├── node.lock
│??             └── _state
│??                 └── global-0.st
└── logs
[root@elk2 elk]# cat elasticsearch/config/elasticsearch.yml 
cluster.name: elkserver
node.name: node-1
path.data: /usr/share/elasticsearch/data
path.logs: /usr/share/elasticsearch/logs
network.host: 0.0.0.0
http.port: 9200
bootstrap.memory_lock: true

docker run --name els -p 9200:9200 -p 9300:9300 -v /docker/elk/elasticsearch/data:/usr/share/elasticsearch/data -v /docker/elk/elasticsearch/config:/usr/share/elasticsearch/config -v /docker/elk/elasticsearch/logs:/usr/share/elasticsearch/logs elasticsearch:2.4.6

#部署kibana
[root@elk2 elk]# tree kibana/
kibana/
└── config
    └── kibana.yml
[root@elk2 config]# grep '^[a-Z]' kibana.yml 
server.port: 5601
server.host: '0.0.0.0'
elasticsearch.url: 'http://elasticsearch:9200'

[root@elk2 config]# docker run -d --name kb --link els:elasticsearch -v /docker/elk/kibana/config:/etc/kibana -p 5601:5601 kibana:4.5

#部署logstash
[root@elk2 elk]# tree logstash/
logstash/
├── config
│?? └── logstash.conf
└── docker-entrypoint.sh
[root@elk2 logstash]# cat config/logstash.conf 
input {
        tcp {
                port => 5000
        }
}
output {
        elasticsearch {
                hosts => "elasticsearch:9200"
                index => "system-log-%{+YYYY.MM.dd}"
        }
}
[root@elk2 logstash]# cat docker-entrypoint.sh 
#!/bin/bash
/opt/logstash/bin/logstash -f /etc/logstash/conf.d

[root@elk2 elasticsearsh]# docker run --name lg -d -p 5000:5000 -v /docker/elk/logstash/config:/etc/logstash/conf.d -v /docker/elk/logstash/docker-entrypoint.sh:/docker-entrypoint.sh --link els:elasticsearch logstash:2.4.0 

#注：--link els:elasticsearch表示连接名称为els的容器，并取一个elasticsearch别名，使在新建的容器中可以解析els主机的ip

</pre>
